{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90614fd-9f3c-4601-96dc-6466ce6b7e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:32:19.772648: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 16:32:19.787478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744216339.802277   77787 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744216339.806858   77787 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744216339.820490   77787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744216339.820507   77787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744216339.820509   77787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744216339.820511   77787 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-09 16:32:19.824658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO: PyTorch version 2.6.0 available.\n",
      "INFO: TensorFlow version 2.19.0 available.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "import contractions\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import whisper\n",
    "from datasets import Dataset\n",
    "from emoji import demojize\n",
    "from pytubefix import YouTube\n",
    "from textblob import TextBlob\n",
    "from tqdm import tqdm\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, TFAutoModelForSeq2SeqLM, Trainer)\n",
    "\n",
    "# Load credentials from environment variables\n",
    "API_BASE = os.getenv(\"LLAMA_API_BASE\", \"http://localhost:8080\")\n",
    "token = os.getenv(\"LLAMA_API_TOKEN\")\n",
    "header = {\n",
    "    \"Authorization\": f\"Bearer {token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "SLANG_DICT = {\n",
    "    # Common abbreviations\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"omg\": \"oh my god\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"idk\": \"i don't know\",\n",
    "    \"tbh\": \"to be honest\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"smh\": \"shaking my head\",\n",
    "    \"afaik\": \"as far as i know\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\",\n",
    "    \"thx\": \"thanks\",\n",
    "    \"pls\": \"please\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"nvm\": \"never mind\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"gtg\": \"got to go\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"dm\": \"direct message\",\n",
    "    \"tmi\": \"too much information\",\n",
    "    # Emphatic expressions\n",
    "    \"wtf\": \"what the fuck\",\n",
    "    \"omfg\": \"oh my fucking god\",\n",
    "    \"stfu\": \"shut the fuck up\",\n",
    "    \"fml\": \"fuck my life\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"lmao\": \"laughing my ass off\",\n",
    "    \"lmfao\": \"laughing my fucking ass off\",\n",
    "    # Modern internet slang\n",
    "    \"sus\": \"suspicious\",\n",
    "    \"ghosting\": \"ignoring someone\",\n",
    "    \"simp\": \"someone idolizing others\",\n",
    "    \"flex\": \"showing off\",\n",
    "    \"clout\": \"influence\",\n",
    "    \"vibe\": \"mood\",\n",
    "    \"yeet\": \"throw forcefully\",\n",
    "    \"lit\": \"exciting\",\n",
    "    \"salty\": \"bitter/angry\",\n",
    "    \"cap\": \"lie\",\n",
    "    \"no cap\": \"truth\",\n",
    "    \"bet\": \"agreement\",\n",
    "    \"ship\": \"relationship\",\n",
    "    \"stan\": \"obsessed fan\",\n",
    "    # Textspeak conversions\n",
    "    \"u\": \"you\",\n",
    "    \"ur\": \"your\",\n",
    "    \"r\": \"are\",\n",
    "    \"y\": \"why\",\n",
    "    \"k\": \"okay\",\n",
    "    \"ppl\": \"people\",\n",
    "    \"def\": \"definitely\",\n",
    "    \"prob\": \"probably\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"gotta\": \"got to\",\n",
    "}\n",
    "SLANG_DICT.update({\n",
    "    \"gg\": \"good game\",\n",
    "    \"op\": \"overpowered\",\n",
    "    \"nerf\": \"reduce power\",\n",
    "    \"pog\": \"awesome\"\n",
    "})\n",
    "\n",
    "\n",
    "def optimized_preprocessor(text):\n",
    "    \"\"\"\n",
    "    Minimal yet effective preprocessing for transformer models\n",
    "    Returns: Cleaned text string\n",
    "    \"\"\"\n",
    "    # Convert emojis to text descriptions\n",
    "    text = demojize(text, delimiters=(\" \", \" \"))\n",
    "\n",
    "    # Expand slang/abbreviations\n",
    "    text = \" \".join([\n",
    "        SLANG_DICT.get(word.lower(), word)\n",
    "        for word in text.split()\n",
    "    ])\n",
    "\n",
    "    # Handle repeated characters (e.g., \"loooool\" ‚Üí \"lool\")\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
    "\n",
    "    # Remove remaining special characters\n",
    "    # (keep apostrophes and basic punctuation)\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s!?,;:'\\-]\", \"\", text)\n",
    "\n",
    "    # Handle contractions (e.g., \"can't\" ‚Üí \"cannot\")\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove user mentions (@username)\n",
    "    text = re.sub(r\"@\\w+\", \"[USER]\", text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"[URL]\", text)\n",
    "\n",
    "    # Normalize numbers\n",
    "    text = re.sub(r\"\\d+\", \"[NUM]\", text)\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_placeholders(text):\n",
    "    \"\"\"\n",
    "    Removes placeholders [NUM], [USER], and [URL] from the text.\n",
    "    \"\"\"\n",
    "    # Remove [NUM], [USER], and [URL]\n",
    "    text = re.sub(r\"\\[NUM\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\[USER\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\[URL\\]\", \"\", text)\n",
    "\n",
    "    # Optionally, strip any extra spaces that might be left after removal\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def transcribe_audio(mp3_file: str, model_name: str = \"base\") -> str:\n",
    "    \"\"\"\n",
    "    Transcribes the given MP3 file using the specified Whisper model.\n",
    "\n",
    "    Args:\n",
    "        mp3_file (str): Path to the MP3 file.\n",
    "        model_name (str): Name of the Whisper model to use\n",
    "                            (tiny, base, small, medium, large).\n",
    "\n",
    "    Returns:\n",
    "        str: The full transcription text.\n",
    "    \"\"\"\n",
    "    print(f\"Loading Whisper model '{model_name}'...\")\n",
    "    model = whisper.load_model(model_name)\n",
    "    print(f\"Transcribing '{mp3_file}'...\")\n",
    "    result = model.transcribe(mp3_file)\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "def split_into_sentences(text: str) -> list:\n",
    "    \"\"\"\n",
    "    Splits a block of text into sentences using a regular expression.\n",
    "    This regex splits the text at punctuation marks (., !, or ?)\n",
    "    followed by whitespace.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to split.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of sentences.\n",
    "    \"\"\"\n",
    "    # The regex splits on punctuation that likely ends a sentence.\n",
    "    sentences = re.split(r\"(?<=[.!?])\\s+\", text)\n",
    "    # Clean up any extra whitespace or empty strings.\n",
    "    sentences = [\n",
    "        sentence.strip()\n",
    "        for sentence in sentences\n",
    "        if sentence.strip()\n",
    "    ]\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def save_sentences_to_csv(sentences: list, output_file: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves a list of sentences into a CSV file with one column \"Sentence\".\n",
    "    Uses UTF-8 encoding with BOM to properly display Romanian characters.\n",
    "\n",
    "    Args:\n",
    "        sentences (list): List of sentence strings.\n",
    "        output_file (str): Path to the output CSV file.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(sentences, columns=[\"Sentence\"])\n",
    "    df.to_csv(\n",
    "        output_file,\n",
    "        index=False,\n",
    "        encoding=\"utf-8-sig\"\n",
    "    )\n",
    "    print(f\"Transcription saved to '{output_file}'.\")\n",
    "\n",
    "\n",
    "def download_youtube_audio(url, destination=\"video_to_mp3\"):\n",
    "    try:\n",
    "        yt = YouTube(url)\n",
    "        title = yt.title\n",
    "        sanitized_title = \"\".join(\n",
    "            c for c in title if c.isalnum() or c in \" _-\"\n",
    "        ).rstrip()\n",
    "        mp3_filename = os.path.join(destination, sanitized_title + \".mp3\")\n",
    "\n",
    "        # Check if the file already exists\n",
    "        if os.path.exists(mp3_filename):\n",
    "            print(f\"File already downloaded: {mp3_filename}\")\n",
    "            return mp3_filename\n",
    "\n",
    "        # If not, download it\n",
    "        video = yt.streams.filter(only_audio=True).first()\n",
    "        out_file = video.download(output_path=destination)\n",
    "        base, ext = os.path.splitext(out_file)\n",
    "        new_file = base + \".mp3\"\n",
    "        if os.path.exists(new_file):\n",
    "            print(f\"File already converted: {new_file}\")\n",
    "            return new_file\n",
    "\n",
    "        os.rename(out_file, new_file)\n",
    "\n",
    "        print(f\"Download complete: {new_file}\")\n",
    "        return new_file\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Failed to download audio:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def handle_mp3_file(path):\n",
    "    if os.path.isfile(path) and path.lower().endswith(\".mp3\"):\n",
    "        print(f\"MP3 file found at: {path}\")\n",
    "        return path\n",
    "    else:\n",
    "        print(\"Invalid file path or not an MP3.\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Translation function using trained model\n",
    "def translate_text(text, tokenizer, model):\n",
    "    tokenized = tokenizer(\n",
    "        [text],\n",
    "        return_tensors=\"np\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "    out = model.generate(**tokenized, max_length=128)\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Function to calculate the absolute polarity\n",
    "def get_abs_polarity(text):\n",
    "    blob = TextBlob(text)\n",
    "    return abs(blob.sentiment.polarity)\n",
    "\n",
    "\n",
    "# Function to categorize intensity based on absolute polarity\n",
    "def categorize_intensity(abs_polarity):\n",
    "    if abs_polarity >= 0.9:\n",
    "        return \"extremely intense\"\n",
    "    elif abs_polarity >= 0.7:\n",
    "        return \"very intense\"\n",
    "    elif abs_polarity >= 0.5:\n",
    "        return \"intense\"\n",
    "    elif abs_polarity >= 0.3:\n",
    "        return \"moderate\"\n",
    "    elif abs_polarity >= 0.1:\n",
    "        return \"mild\"\n",
    "    elif abs_polarity >= 0.05:\n",
    "        return \"slightly mild\"\n",
    "    else:\n",
    "        return \"low\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cbb7e0-bbe6-4359-8e51-f05d343635b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a YouTube link or path to an .mp3 file:\n",
      " https://www.youtube.com/watch?v=K0Lvtkn-xfw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already converted: /home/y2c/video_to_mp3/SƒÉ √ÆnceapƒÉ jocul de recompensƒÉ | SURVIVOR 2025.mp3\n",
      "File path: /home/y2c/video_to_mp3/SƒÉ √ÆnceapƒÉ jocul de recompensƒÉ | SURVIVOR 2025.mp3\n",
      "Loading Whisper model 'large'...\n",
      "Transcribing '/home/y2c/video_to_mp3/SƒÉ √ÆnceapƒÉ jocul de recompensƒÉ | SURVIVOR 2025.mp3'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üîç Correcting transcriptions with LLaMA: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1094/1094 [08:57<00:00,  2.04it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription saved to 'audio_transcript.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:58:45.843352: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "I0000 00:00:1744217925.843732   77787 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 33966 MB memory:  -> device: 0, name: NVIDIA L40S, pci bus id: 0000:01:00.0, compute capability: 8.9\n",
      "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
      "\n",
      "All the layers of TFMarianMTModel were initialized from the model checkpoint at models/models/translation/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated sentences saved to 'translated_data.csv'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58dfd1531564e56a1f2c06d776d4aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77787/3673794936.py:124: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14012e8440a74b38a75133231d93b0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1094 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_77787/3673794936.py:182: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Corrected_Sentence</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Preprocessed_Translation</th>\n",
       "      <th>Core_Emotion</th>\n",
       "      <th>Fine_Emotion</th>\n",
       "      <th>Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StƒÉp!</td>\n",
       "      <td>StƒÉp!</td>\n",
       "      <td>Master!</td>\n",
       "      <td>master!</td>\n",
       "      <td>surprise</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>»òi √Æncep cei doi, iatƒÉ √Æn acest moment.</td>\n",
       "      <td>»òi √Æncep cei doi, iatƒÉ √Æn acest moment.</td>\n",
       "      <td>And start the two, here we go right now.</td>\n",
       "      <td>and start the two, here we go right now</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drag foarte tare cƒÉtre acele stƒÉgule»õe.</td>\n",
       "      <td>Dragoste foarte mare cƒÉtre acele stelu»õe.</td>\n",
       "      <td>Very big love to those stars.</td>\n",
       "      <td>very big love to those stars</td>\n",
       "      <td>happiness</td>\n",
       "      <td>peaceful</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RidicƒÉ Adi, au fost aproape cei doi.</td>\n",
       "      <td>RidicƒÉ, Adi, au fost aproape cei doi.</td>\n",
       "      <td>Get up, Adi, it was close to the two of us.</td>\n",
       "      <td>get up, adi, it was close to the two of us</td>\n",
       "      <td>fear</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.</td>\n",
       "      <td>Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.</td>\n",
       "      <td>They put resistance on the boys' line.</td>\n",
       "      <td>they put resistance on the boys' line</td>\n",
       "      <td>disgust</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089</th>\n",
       "      <td>Bravo ceilalte!</td>\n",
       "      <td>Bravo celorlalte!</td>\n",
       "      <td>Good for the others!</td>\n",
       "      <td>good for the others!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>optimistic</td>\n",
       "      <td>very intense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>Esti important!</td>\n",
       "      <td>Esti important!</td>\n",
       "      <td>You're important!</td>\n",
       "      <td>you are important!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>optimistic</td>\n",
       "      <td>intense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1091</th>\n",
       "      <td>rifinizƒÉ-te!</td>\n",
       "      <td>ResfinizeazƒÉ-te!</td>\n",
       "      <td>Refine yourself!</td>\n",
       "      <td>refine yourself!</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092</th>\n",
       "      <td>Idol Ne removed Once colossi ≈üi critical mosqu...</td>\n",
       "      <td>Idol Ne removed Once colossi ≈üi critical mosqu...</td>\n",
       "      <td>Idol We removed Once colossi and critical musc...</td>\n",
       "      <td>idol we removed once colossi and critical musc...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1093</th>\n",
       "      <td>Dev' cops Billion StorVE SƒÉ vƒÉ mul»õumim pentru...</td>\n",
       "      <td>Dev' cops Billion StorVE SƒÉ vƒÉ mul»õumim pentru...</td>\n",
       "      <td>Dev' cops Billion Storm Let's thank you for wa...</td>\n",
       "      <td>dev' cops billion storm let us thank you for w...</td>\n",
       "      <td>happiness</td>\n",
       "      <td>peaceful</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1094 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0                                                 StƒÉp!   \n",
       "1               »òi √Æncep cei doi, iatƒÉ √Æn acest moment.   \n",
       "2               Drag foarte tare cƒÉtre acele stƒÉgule»õe.   \n",
       "3                  RidicƒÉ Adi, au fost aproape cei doi.   \n",
       "4                 Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.   \n",
       "...                                                 ...   \n",
       "1089                                    Bravo ceilalte!   \n",
       "1090                                    Esti important!   \n",
       "1091                                       rifinizƒÉ-te!   \n",
       "1092  Idol Ne removed Once colossi ≈üi critical mosqu...   \n",
       "1093  Dev' cops Billion StorVE SƒÉ vƒÉ mul»õumim pentru...   \n",
       "\n",
       "                                     Corrected_Sentence  \\\n",
       "0                                                 StƒÉp!   \n",
       "1               »òi √Æncep cei doi, iatƒÉ √Æn acest moment.   \n",
       "2             Dragoste foarte mare cƒÉtre acele stelu»õe.   \n",
       "3                 RidicƒÉ, Adi, au fost aproape cei doi.   \n",
       "4                 Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.   \n",
       "...                                                 ...   \n",
       "1089                                  Bravo celorlalte!   \n",
       "1090                                    Esti important!   \n",
       "1091                                   ResfinizeazƒÉ-te!   \n",
       "1092  Idol Ne removed Once colossi ≈üi critical mosqu...   \n",
       "1093  Dev' cops Billion StorVE SƒÉ vƒÉ mul»õumim pentru...   \n",
       "\n",
       "                                            Translation  \\\n",
       "0                                               Master!   \n",
       "1              And start the two, here we go right now.   \n",
       "2                         Very big love to those stars.   \n",
       "3           Get up, Adi, it was close to the two of us.   \n",
       "4                They put resistance on the boys' line.   \n",
       "...                                                 ...   \n",
       "1089                               Good for the others!   \n",
       "1090                                  You're important!   \n",
       "1091                                   Refine yourself!   \n",
       "1092  Idol We removed Once colossi and critical musc...   \n",
       "1093  Dev' cops Billion Storm Let's thank you for wa...   \n",
       "\n",
       "                               Preprocessed_Translation Core_Emotion  \\\n",
       "0                                               master!     surprise   \n",
       "1               and start the two, here we go right now      neutral   \n",
       "2                          very big love to those stars    happiness   \n",
       "3            get up, adi, it was close to the two of us         fear   \n",
       "4                 they put resistance on the boys' line      disgust   \n",
       "...                                                 ...          ...   \n",
       "1089                               good for the others!    happiness   \n",
       "1090                                 you are important!    happiness   \n",
       "1091                                   refine yourself!    happiness   \n",
       "1092  idol we removed once colossi and critical musc...    happiness   \n",
       "1093  dev' cops billion storm let us thank you for w...    happiness   \n",
       "\n",
       "     Fine_Emotion     Intensity  \n",
       "0         neutral           low  \n",
       "1         neutral          mild  \n",
       "2        peaceful          mild  \n",
       "3         neutral           low  \n",
       "4         neutral           low  \n",
       "...           ...           ...  \n",
       "1089   optimistic  very intense  \n",
       "1090   optimistic       intense  \n",
       "1091      neutral           low  \n",
       "1092      neutral           low  \n",
       "1093     peaceful           low  \n",
       "\n",
       "[1094 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = input(\"Enter a YouTube link or path to an .mp3 file:\\n\").strip()\n",
    "if user_input.startswith(\"http://\") or user_input.startswith(\"https://\"):\n",
    "    file_path = download_youtube_audio(user_input)\n",
    "else:\n",
    "    file_path = handle_mp3_file(user_input)\n",
    "\n",
    "if not file_path or not os.path.isfile(file_path):\n",
    "    print(f\"Error: The file '{file_path}' does not exist.\")\n",
    "    exit(1)\n",
    "print(f\"File path: {file_path}\")\n",
    "# Transcribe the audio file.\n",
    "transcription_text = transcribe_audio(file_path, model_name='large')\n",
    "\n",
    "# Split the transcription into sentences.\n",
    "\n",
    "sentences = split_into_sentences(transcription_text)\n",
    "# Romanian Transcription Correction Prompt\n",
    "system_prompt_romanian = \"\"\"\n",
    "You are an assistant that helps correct transcription errors in Romanian. Below is a sentence. If there are any mistakes, correct only the spelling mistakes ‚Äî do not replace words with synonyms or change their meaning. Keep the original words unless there is a clear spelling or transcription error. Do not rephrase or rewrite. Only output the corrected sentence.\n",
    "\n",
    "Sentence:\n",
    "\"\"\"\n",
    "\n",
    "# Helper function for API call\n",
    "def correct_transcription_romanian(text, system_prompt, api_url, headers):\n",
    "    payload = {\n",
    "        \"model\": \"llama3.3:latest\",  # Example: Adjust model if needed\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": text}\n",
    "        ],\n",
    "        \"options\": {\"num_ctx\": 4096 * 2},\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_p\": 0.2\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(f\"{api_url}/api/chat/completions\", headers=headers, json=payload, timeout=180)\n",
    "        result = response.json()\n",
    "        choices = result.get(\"choices\", [])\n",
    "        if choices and \"message\" in choices[0]:\n",
    "            return choices[0][\"message\"][\"content\"].strip()\n",
    "        else:\n",
    "            return \"error\"\n",
    "    except Exception as e:\n",
    "        return \"error\"\n",
    "\n",
    "# Process sentences and correct transcriptions\n",
    "def correct_sentences(sentences, system_prompt, api_url, headers):\n",
    "    corrected_sentences = [None] * len(sentences)  # List to store corrected sentences in order\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=200) as executor:\n",
    "        futures = {\n",
    "            executor.submit(correct_transcription_romanian, sentence, system_prompt, api_url, headers): idx\n",
    "            for idx, sentence in enumerate(sentences)\n",
    "        }\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"üîç Correcting transcriptions with LLaMA\"):\n",
    "            idx = futures[future]\n",
    "            try:\n",
    "                corrected = future.result()\n",
    "                if corrected != \"error\":\n",
    "                    original_words = sentences[idx].strip().split()\n",
    "                    corrected_words = corrected.strip().split()\n",
    "                \n",
    "                    # If difference in word count is small enough, accept it\n",
    "                    if abs(len(corrected_words) - len(original_words)) <= 2:\n",
    "                        corrected_sentences[idx] = corrected\n",
    "                    else:\n",
    "                        corrected_sentences[idx] = sentences[idx]  # Keep original\n",
    "                else:\n",
    "                    corrected_sentences[idx] = sentences[idx]  # On error, keep original\n",
    "            except Exception as e:\n",
    "                corrected_sentences[idx] = sentences[idx]  # Keep original sentence in case of error\n",
    "\n",
    "    return corrected_sentences\n",
    "\n",
    "# Correct the sentences\n",
    "corrected_sentences = correct_sentences(sentences, system_prompt_romanian, API_BASE, header)\n",
    "\n",
    "# Determine output file name\n",
    "output_file = \"audio_transcript.csv\"\n",
    "\n",
    "# Save the sentences to CSV.\n",
    "save_sentences_to_csv(sentences, output_file)\n",
    "# Reload trained model for translation\n",
    "tokenizer_translate = AutoTokenizer.from_pretrained(\"models/models/translation/\")\n",
    "# Force the model to run on CPU\n",
    "with tf.device('/cpu:0'):\n",
    "    model_translate = TFAutoModelForSeq2SeqLM.from_pretrained(\"models/models/translation/\")\n",
    "df = pd.read_csv(\"audio_transcript.csv\")\n",
    "df['Corrected_Sentence'] = corrected_sentences\n",
    "# Apply translation using trained model\n",
    "df[\"Translation\"] = df[\"Corrected_Sentence\"].astype(str).apply(\n",
    "    lambda x: translate_text(x, tokenizer=tokenizer_translate, model=model_translate)\n",
    ")\n",
    "\n",
    "# Save the result as an Excel file\n",
    "df.to_csv(\"translated_data.csv\", index=False,encoding='utf-8-sig')\n",
    "print(f\"Translated sentences saved to 'translated_data.csv'.\")\n",
    "\n",
    "\n",
    "model_path = r\"models/models/emotion_7\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "df['Preprocessed_Translation'] = df['Translation'].apply(optimized_preprocessor)\n",
    "df['Preprocessed_Translation'] = df['Preprocessed_Translation'].apply(remove_placeholders)\n",
    "# Load or define df_test here\n",
    "dataset = Dataset.from_pandas(df[['Preprocessed_Translation']])\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['Preprocessed_Translation'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset_test = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = tokenized_dataset_test.remove_columns([\"Preprocessed_Translation\"])\n",
    "tokenized_dataset_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "# Recreate the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "predictions_output = trainer.predict(tokenized_dataset_test)\n",
    "\n",
    "# Extract logits and labels\n",
    "logits = predictions_output.predictions\n",
    "\n",
    "# Convert logits to predicted label indices\n",
    "predicted_labels = logits.argmax(axis=1)\n",
    "\n",
    "# Define label mappings happiness - sadness - anger - surprise - fear - disgust\n",
    "label_mapping = {\n",
    "    'disgust': 0,\n",
    "    'happiness': 1,\n",
    "    'anger': 2,\n",
    "    'neutral': 3,\n",
    "    'sadness': 4,\n",
    "    'fear': 5,\n",
    "    'surprise': 6\n",
    "}\n",
    "# Reverse mapping to decode label integers\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "df['Core_Emotion'] = predicted_labels\n",
    "df['Core_Emotion'] = df['Core_Emotion'].map(reverse_label_mapping)\n",
    "\n",
    "label_mapping_fine = {'accepted': 0, \n",
    "                      'angry': 1, \n",
    "                      'confused': 2, \n",
    "                      'excited': 3, \n",
    "                      'fearful': 4, \n",
    "                      'frustrated': 5, \n",
    "                      'grossed out': 6, \n",
    "                      'guilty': 7, \n",
    "                      'humiliated': 8, \n",
    "                      'hurt': 9, \n",
    "                      'interested': 10, \n",
    "                      'joyful': 11, \n",
    "                      'nervous': 12, \n",
    "                      'neutral': 13, \n",
    "                      'optimistic': 14, \n",
    "                      'outraged': 15, \n",
    "                      'peaceful': 16, \n",
    "                      'proud': 17, \n",
    "                      'startled': 18}\n",
    "\n",
    "model_path = r\"models/models/emotion_19\"\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenized_dataset_test = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = tokenized_dataset_test.remove_columns([\"Preprocessed_Translation\"])\n",
    "tokenized_dataset_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "# Recreate the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "predictions_output = trainer.predict(tokenized_dataset_test)\n",
    "\n",
    "# Extract logits and labels\n",
    "logits = predictions_output.predictions\n",
    "\n",
    "# Convert logits to predicted label indices\n",
    "predicted_labels = logits.argmax(axis=1)\n",
    "# Reverse mapping to decode label integers\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping_fine.items()}\n",
    "df['Fine_Emotion'] = predicted_labels\n",
    "df['Fine_Emotion'] = df['Fine_Emotion'].map(reverse_label_mapping)\n",
    "\n",
    "\n",
    "# Apply the function to the sentences column\n",
    "df['Intensity'] = df['Preprocessed_Translation'].apply(get_abs_polarity)\n",
    "# Categorize the intensity based on absolute polarity\n",
    "df['Intensity'] = df['Intensity'].apply(categorize_intensity)\n",
    "df.to_csv(f\"pipeline_output_1.csv\", index=False, encoding='utf-8-sig')\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150b9284-1977-4a3e-b3f3-cb9f6769b703",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=WNDVIthCYeQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd8222-bc62-46e0-ac77-84ab2a93268a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d265c9a-7aed-4393-b1b8-f9006febf0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Corrected_Sentence</th>\n",
       "      <th>Translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StƒÉp!</td>\n",
       "      <td>StƒÉp!</td>\n",
       "      <td>Master!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>»òi √Æncep cei doi, iatƒÉ √Æn acest moment.</td>\n",
       "      <td>»òi √Æncep cei doi, iatƒÉ √Æn acest moment.</td>\n",
       "      <td>And start the two, here we go right now.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drag foarte tare cƒÉtre acele stƒÉgule»õe.</td>\n",
       "      <td>Dragostea este foarte mare cƒÉtre acele steagur...</td>\n",
       "      <td>Love is very big to those small flags.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RidicƒÉ Adi, au fost aproape cei doi.</td>\n",
       "      <td>RidicƒÉ Adi, au fost aproape cei doi.</td>\n",
       "      <td>Pick up Adi, it was almost the two of them.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.</td>\n",
       "      <td>Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.</td>\n",
       "      <td>They put resistance on the boys' line.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>Nu-i, nu-i!</td>\n",
       "      <td>Nu-i, nu-i!</td>\n",
       "      <td>It's not, it's not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ha!</td>\n",
       "      <td>Ha!</td>\n",
       "      <td>Ha!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>Tu stiai ca esti atat de forta?</td>\n",
       "      <td>Tu »ôtiai cƒÉ e»ôti at√¢t de forte?</td>\n",
       "      <td>You knew you were that strong?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>Abar n-aveai!</td>\n",
       "      <td>Aber n-aveai!</td>\n",
       "      <td>Aber didn't have any!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>Astazi Greucianul a amintit pe zmeul Filip!</td>\n",
       "      <td>Astorazi grecianul a amintit de zmeul Filip!</td>\n",
       "      <td>Astorazi the Greek remembered the rye Philip!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sentence  \\\n",
       "0                                           StƒÉp!   \n",
       "1         »òi √Æncep cei doi, iatƒÉ √Æn acest moment.   \n",
       "2         Drag foarte tare cƒÉtre acele stƒÉgule»õe.   \n",
       "3            RidicƒÉ Adi, au fost aproape cei doi.   \n",
       "4           Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.   \n",
       "...                                           ...   \n",
       "1159                                  Nu-i, nu-i!   \n",
       "1160                                          Ha!   \n",
       "1161              Tu stiai ca esti atat de forta?   \n",
       "1162                                Abar n-aveai!   \n",
       "1163  Astazi Greucianul a amintit pe zmeul Filip!   \n",
       "\n",
       "                                     Corrected_Sentence  \\\n",
       "0                                                 StƒÉp!   \n",
       "1               »òi √Æncep cei doi, iatƒÉ √Æn acest moment.   \n",
       "2     Dragostea este foarte mare cƒÉtre acele steagur...   \n",
       "3                  RidicƒÉ Adi, au fost aproape cei doi.   \n",
       "4                 Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.   \n",
       "...                                                 ...   \n",
       "1159                                        Nu-i, nu-i!   \n",
       "1160                                                Ha!   \n",
       "1161                    Tu »ôtiai cƒÉ e»ôti at√¢t de forte?   \n",
       "1162                                      Aber n-aveai!   \n",
       "1163       Astorazi grecianul a amintit de zmeul Filip!   \n",
       "\n",
       "                                        Translation  \n",
       "0                                           Master!  \n",
       "1          And start the two, here we go right now.  \n",
       "2            Love is very big to those small flags.  \n",
       "3       Pick up Adi, it was almost the two of them.  \n",
       "4            They put resistance on the boys' line.  \n",
       "...                                             ...  \n",
       "1159                            It's not, it's not!  \n",
       "1160                                            Ha!  \n",
       "1161                 You knew you were that strong?  \n",
       "1162                          Aber didn't have any!  \n",
       "1163  Astorazi the Greek remembered the rye Philip!  \n",
       "\n",
       "[1164 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv('translated_data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcfbaaa8-b7c0-45a0-b987-d15fa0c0e520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3fdef4c60849dab1a0c8043e6dc408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73050/4212451721.py:22: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa2b90cbfc6469e9fa313ced88d19fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1164 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_73050/4212451721.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Corrected_Sentence</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Preprocessed_Translation</th>\n",
       "      <th>Core_Emotion</th>\n",
       "      <th>Fine_Emotion</th>\n",
       "      <th>Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>StƒÉp!</td>\n",
       "      <td>StƒÉp!</td>\n",
       "      <td>Master!</td>\n",
       "      <td>master!</td>\n",
       "      <td>surprise</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>»òi √Æncep cei doi, iatƒÉ √Æn acest moment.</td>\n",
       "      <td>»òi √Æncep cei doi, iatƒÉ √Æn acest moment.</td>\n",
       "      <td>And start the two, here we go right now.</td>\n",
       "      <td>and start the two, here we go right now</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drag foarte tare cƒÉtre acele stƒÉgule»õe.</td>\n",
       "      <td>Dragostea este foarte mare cƒÉtre acele steagur...</td>\n",
       "      <td>Love is very big to those small flags.</td>\n",
       "      <td>love is very big to those small flags</td>\n",
       "      <td>happiness</td>\n",
       "      <td>peaceful</td>\n",
       "      <td>slightly mild</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RidicƒÉ Adi, au fost aproape cei doi.</td>\n",
       "      <td>RidicƒÉ Adi, au fost aproape cei doi.</td>\n",
       "      <td>Pick up Adi, it was almost the two of them.</td>\n",
       "      <td>pick up adi, it was almost the two of them</td>\n",
       "      <td>surprise</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.</td>\n",
       "      <td>Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.</td>\n",
       "      <td>They put resistance on the boys' line.</td>\n",
       "      <td>they put resistance on the boys' line</td>\n",
       "      <td>disgust</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>Nu-i, nu-i!</td>\n",
       "      <td>Nu-i, nu-i!</td>\n",
       "      <td>It's not, it's not!</td>\n",
       "      <td>it is not, it is not!</td>\n",
       "      <td>disgust</td>\n",
       "      <td>outraged</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>Ha!</td>\n",
       "      <td>Ha!</td>\n",
       "      <td>Ha!</td>\n",
       "      <td>ha!</td>\n",
       "      <td>surprise</td>\n",
       "      <td>excited</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>Tu stiai ca esti atat de forta?</td>\n",
       "      <td>Tu »ôtiai cƒÉ e»ôti at√¢t de forte?</td>\n",
       "      <td>You knew you were that strong?</td>\n",
       "      <td>you knew you were that strong?</td>\n",
       "      <td>happiness</td>\n",
       "      <td>interested</td>\n",
       "      <td>moderate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1162</th>\n",
       "      <td>Abar n-aveai!</td>\n",
       "      <td>Aber n-aveai!</td>\n",
       "      <td>Aber didn't have any!</td>\n",
       "      <td>aber did not have any!</td>\n",
       "      <td>surprise</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>Astazi Greucianul a amintit pe zmeul Filip!</td>\n",
       "      <td>Astorazi grecianul a amintit de zmeul Filip!</td>\n",
       "      <td>Astorazi the Greek remembered the rye Philip!</td>\n",
       "      <td>astorazi the greek remembered the rye philip!</td>\n",
       "      <td>surprise</td>\n",
       "      <td>neutral</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1164 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Sentence  \\\n",
       "0                                           StƒÉp!   \n",
       "1         »òi √Æncep cei doi, iatƒÉ √Æn acest moment.   \n",
       "2         Drag foarte tare cƒÉtre acele stƒÉgule»õe.   \n",
       "3            RidicƒÉ Adi, au fost aproape cei doi.   \n",
       "4           Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.   \n",
       "...                                           ...   \n",
       "1159                                  Nu-i, nu-i!   \n",
       "1160                                          Ha!   \n",
       "1161              Tu stiai ca esti atat de forta?   \n",
       "1162                                Abar n-aveai!   \n",
       "1163  Astazi Greucianul a amintit pe zmeul Filip!   \n",
       "\n",
       "                                     Corrected_Sentence  \\\n",
       "0                                                 StƒÉp!   \n",
       "1               »òi √Æncep cei doi, iatƒÉ √Æn acest moment.   \n",
       "2     Dragostea este foarte mare cƒÉtre acele steagur...   \n",
       "3                  RidicƒÉ Adi, au fost aproape cei doi.   \n",
       "4                 Au pus rezisten»õƒÉ pe linia bƒÉie»õilor.   \n",
       "...                                                 ...   \n",
       "1159                                        Nu-i, nu-i!   \n",
       "1160                                                Ha!   \n",
       "1161                    Tu »ôtiai cƒÉ e»ôti at√¢t de forte?   \n",
       "1162                                      Aber n-aveai!   \n",
       "1163       Astorazi grecianul a amintit de zmeul Filip!   \n",
       "\n",
       "                                        Translation  \\\n",
       "0                                           Master!   \n",
       "1          And start the two, here we go right now.   \n",
       "2            Love is very big to those small flags.   \n",
       "3       Pick up Adi, it was almost the two of them.   \n",
       "4            They put resistance on the boys' line.   \n",
       "...                                             ...   \n",
       "1159                            It's not, it's not!   \n",
       "1160                                            Ha!   \n",
       "1161                 You knew you were that strong?   \n",
       "1162                          Aber didn't have any!   \n",
       "1163  Astorazi the Greek remembered the rye Philip!   \n",
       "\n",
       "                           Preprocessed_Translation Core_Emotion Fine_Emotion  \\\n",
       "0                                           master!     surprise      neutral   \n",
       "1           and start the two, here we go right now      neutral      neutral   \n",
       "2             love is very big to those small flags    happiness     peaceful   \n",
       "3        pick up adi, it was almost the two of them     surprise      neutral   \n",
       "4             they put resistance on the boys' line      disgust      neutral   \n",
       "...                                             ...          ...          ...   \n",
       "1159                          it is not, it is not!      disgust     outraged   \n",
       "1160                                            ha!     surprise      excited   \n",
       "1161                 you knew you were that strong?    happiness   interested   \n",
       "1162                         aber did not have any!     surprise      neutral   \n",
       "1163  astorazi the greek remembered the rye philip!     surprise      neutral   \n",
       "\n",
       "          Intensity  \n",
       "0               low  \n",
       "1              mild  \n",
       "2     slightly mild  \n",
       "3               low  \n",
       "4               low  \n",
       "...             ...  \n",
       "1159            low  \n",
       "1160            low  \n",
       "1161       moderate  \n",
       "1162            low  \n",
       "1163            low  \n",
       "\n",
       "[1164 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = r\"models/models/emotion_7\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "# Move model to device (GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "df['Preprocessed_Translation'] = df['Translation'].apply(optimized_preprocessor)\n",
    "df['Preprocessed_Translation'] = df['Preprocessed_Translation'].apply(remove_placeholders)\n",
    "# Load or define df_test here\n",
    "dataset = Dataset.from_pandas(df[['Preprocessed_Translation']])\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example['Preprocessed_Translation'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "tokenized_dataset_test = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = tokenized_dataset_test.remove_columns([\"Preprocessed_Translation\"])\n",
    "tokenized_dataset_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "# Recreate the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "predictions_output = trainer.predict(tokenized_dataset_test)\n",
    "\n",
    "# Extract logits and labels\n",
    "logits = predictions_output.predictions\n",
    "\n",
    "# Convert logits to predicted label indices\n",
    "predicted_labels = logits.argmax(axis=1)\n",
    "\n",
    "# Define label mappings happiness - sadness - anger - surprise - fear - disgust\n",
    "label_mapping = {\n",
    "    'disgust': 0,\n",
    "    'happiness': 1,\n",
    "    'anger': 2,\n",
    "    'neutral': 3,\n",
    "    'sadness': 4,\n",
    "    'fear': 5,\n",
    "    'surprise': 6\n",
    "}\n",
    "# Reverse mapping to decode label integers\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n",
    "\n",
    "df['Core_Emotion'] = predicted_labels\n",
    "df['Core_Emotion'] = df['Core_Emotion'].map(reverse_label_mapping)\n",
    "\n",
    "label_mapping_fine = {'accepted': 0, \n",
    "                      'angry': 1, \n",
    "                      'confused': 2, \n",
    "                      'excited': 3, \n",
    "                      'fearful': 4, \n",
    "                      'frustrated': 5, \n",
    "                      'grossed out': 6, \n",
    "                      'guilty': 7, \n",
    "                      'humiliated': 8, \n",
    "                      'hurt': 9, \n",
    "                      'interested': 10, \n",
    "                      'joyful': 11, \n",
    "                      'nervous': 12, \n",
    "                      'neutral': 13, \n",
    "                      'optimistic': 14, \n",
    "                      'outraged': 15, \n",
    "                      'peaceful': 16, \n",
    "                      'proud': 17, \n",
    "                      'startled': 18}\n",
    "\n",
    "model_path = r\"models/models/emotion_19\"\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "tokenized_dataset_test = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset_test = tokenized_dataset_test.remove_columns([\"Preprocessed_Translation\"])\n",
    "tokenized_dataset_test.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "# Recreate the trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "predictions_output = trainer.predict(tokenized_dataset_test)\n",
    "\n",
    "# Extract logits and labels\n",
    "logits = predictions_output.predictions\n",
    "\n",
    "# Convert logits to predicted label indices\n",
    "predicted_labels = logits.argmax(axis=1)\n",
    "# Reverse mapping to decode label integers\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping_fine.items()}\n",
    "df['Fine_Emotion'] = predicted_labels\n",
    "df['Fine_Emotion'] = df['Fine_Emotion'].map(reverse_label_mapping)\n",
    "\n",
    "\n",
    "# Apply the function to the sentences column\n",
    "df['Intensity'] = df['Preprocessed_Translation'].apply(get_abs_polarity)\n",
    "# Categorize the intensity based on absolute polarity\n",
    "df['Intensity'] = df['Intensity'].apply(categorize_intensity)\n",
    "df.to_csv(f\"pipeline_output.csv\", index=False, encoding='utf-8-sig')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9677c49-d2c4-4a0c-b68e-4f0ff26519de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
