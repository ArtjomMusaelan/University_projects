{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example code for Task 5: Model Iterations.\n",
    "This script demonstrates how to:\n",
    "1. Load a CSV file containing NLP features (from Task 4).\n",
    "2. Parse columns such as TF-IDF, sentiment scores, custom embeddings, etc.\n",
    "3. Encode the target emotion labels.\n",
    "4. Train and evaluate:\n",
    "   - Logistic Regression\n",
    "   - Naive Bayes\n",
    "   - LSTM (Keras/TensorFlow)\n",
    "   - RNN (Keras/TensorFlow)\n",
    "   - Two Transformer models (via Hugging Face)\n",
    "\n",
    "You should adapt paths, filenames, column names, and any language-specific libraries\n",
    "to match your actual data and environment.\n",
    "\n",
    "Ensure you record each model's details (features used, hyperparameters, metrics)\n",
    "in your model iteration file (required by Task 5).\n",
    "\n",
    "---\n",
    "## INSTALL REQUIRED LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 13:02:32.785649: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 13:02:32.824282: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-09 13:02:32.824315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-09 13:02:32.825796: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 13:02:32.832447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO: PyTorch version 2.6.0 available.\n",
      "INFO: TensorFlow version 2.15.1 available.\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn numpy pandas tensorflow torch transformers sentencepiece gensim datasets\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Sklearn for classic models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertModel, BertConfig, AutoModel\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow/Keras for LSTM, RNN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense, Dropout, Input, Add, Bidirectional\n",
    "\n",
    "# Hugging Face Transformers\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorWithPadding\n",
    "import transformers\n",
    "import keras\n",
    "\n",
    "NUM_TEST_SAMPLES = 1044\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame shape after dropping NaN values: (87267, 3)\n",
      "DataFrame columns: ['ru_text', 'text', 'general_emotion']\n",
      "Emotion classes: ['anger' 'disgust' 'fear' 'happiness' 'neutral' 'sadness' 'surprise']\n",
      "Test shape: (1044, 2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6712c35000f42e5aef73a563e1490fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/1044 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, RobertaTokenizer, RobertaForSequenceClassification, RobertaConfig\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import EarlyStoppingCallback\n",
    "import evaluate\n",
    "\n",
    "df = pd.read_parquet('group_combined_no_features.parquet')\n",
    "print(f\"DataFrame shape after dropping NaN values: {df.shape}\")\n",
    "print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"emotion_encoded\"] = label_encoder.fit_transform(df[\"general_emotion\"])\n",
    "classes_ = label_encoder.classes_\n",
    "num_classes = len(classes_)\n",
    "print(\"Emotion classes:\", classes_)\n",
    "\n",
    "dataframe_4_bert = df[['text', 'emotion_encoded']]\n",
    "\n",
    "test_df = dataframe_4_bert[-NUM_TEST_SAMPLES:].reset_index(drop=True)\n",
    "\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Combine into DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"distilbert/distilroberta-base\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Tokenize dataset with progress bar (using tqdm)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True, desc=\"Tokenizing\")\n",
    "\n",
    "# Convert dataset format\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_datasets['test'][12]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load your fine-tuned model\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"./distilroberta_finetuned_v2\")\n",
    "\n",
    "# Use the Trainer for prediction\n",
    "trainer = Trainer(model=model)\n",
    "trainer.args.report_to = \"none\"  # Disable logging to avoid unnecessary output\n",
    "\n",
    "# Predict\n",
    "predictions = trainer.predict(tokenized_datasets[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No columns in the dataset match the model's forward method signature. The following columns have been ignored: [emotion_encoded, text]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Get predictions\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions\u001b[38;5;241m.\u001b[39mpredictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mlabel_ids\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.9/site-packages/transformers/trainer.py:4210\u001b[0m, in \u001b[0;36mTrainer.predict\u001b[0;34m(self, test_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4207\u001b[0m \u001b[38;5;66;03m# memory metrics - must set up as early as possible\u001b[39;00m\n\u001b[1;32m   4208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_tracker\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m-> 4210\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_test_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4211\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4213\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.9/site-packages/transformers/trainer.py:1153\u001b[0m, in \u001b[0;36mTrainer.get_test_dataloader\u001b[0;34m(self, test_dataset)\u001b[0m\n\u001b[1;32m   1150\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator\n\u001b[1;32m   1152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(test_dataset, datasets\u001b[38;5;241m.\u001b[39mDataset):\n\u001b[0;32m-> 1153\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remove_unused_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m     data_collator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_collator_with_removed_columns(data_collator, description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_env/lib/python3.9/site-packages/transformers/trainer.py:939\u001b[0m, in \u001b[0;36mTrainer._remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    937\u001b[0m columns \u001b[38;5;241m=\u001b[39m [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature_columns \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcolumn_names]\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 939\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    940\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo columns in the dataset match the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms forward method signature. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following columns have been ignored: [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(ignored_columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    943\u001b[0m     )\n\u001b[1;32m    945\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(datasets\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.4.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    946\u001b[0m     dataset\u001b[38;5;241m.\u001b[39mset_format(\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m], columns\u001b[38;5;241m=\u001b[39mcolumns, format_kwargs\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mformat[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    948\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [emotion_encoded, text]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# Get predictions\n",
    "predictions = trainer.predict(test_dataset)\n",
    "preds = np.argmax(predictions.predictions, axis=1)\n",
    "true_labels = predictions.label_ids\n",
    "print(classification_report(true_labels, preds, target_names=classes_))\n",
    "\n",
    "# Confsion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(true_labels, preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=classes_, yticklabels=classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 12: COLLECT AND PRINT FINAL RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== FINAL SUMMARY ===\")\n",
    "print(f\"Logistic Regression F1: {f1_lr:.4f}\")\n",
    "print(f\"Naive Bayes F1: {f1_nb:.4f}\")\n",
    "print(f\"LSTM F1: {f1_lstm:.4f}\")\n",
    "print(f\"RNN F1: {f1_rnn:.4f}\")\n",
    "print(f\"BERT F1: {eval_results_bert['eval_f1_macro']:.4f}\")\n",
    "print(f\"DistilBERT F1: {eval_results_distil['eval_f1_macro']:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "NOTE: \n",
    "1. For each model/iteration, record details (features used, hyperparameters, F1-score, comments) \n",
    "   in your model iteration file.\n",
    "2. Feel free to adjust hyperparameters (batch size, number of epochs, etc.) to improve results.\n",
    "3. You may also incorporate more features (POS tags, Pretrained_Embeddings, etc.) \n",
    "   by modifying the make_feature_vector() function or building advanced architectures.\n",
    "4. This example focuses on demonstrating the core steps; you must tailor it to your specific project setup.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp_env)",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
