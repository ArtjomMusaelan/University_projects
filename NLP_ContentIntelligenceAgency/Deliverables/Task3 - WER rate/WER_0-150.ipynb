{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nikitaorlov/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 34\n",
      "Total tokens in original: 1402.0\n",
      "WER: 0.024251069900142655\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def get_sid(reference_tokens, hypothesis_tokens):\n",
    "    \"\"\"\n",
    "    Calculate Substitutions (S), Insertions (I), and Deletions (D)\n",
    "    between reference and hypothesis token lists.\n",
    "    \"\"\"\n",
    "\n",
    "    matcher = Levenshtein.editops(reference_tokens, hypothesis_tokens)\n",
    "    \n",
    "    S, I, D = 0, 0, 0\n",
    "    for op in matcher:\n",
    "        if op[0] == 'replace':\n",
    "            S += 1\n",
    "        elif op[0] == 'insert':\n",
    "            I += 1\n",
    "        elif op[0] == 'delete':\n",
    "            D += 1\n",
    "    \n",
    "    return S, I, D\n",
    "\n",
    "def prepare_table_for_wer(data_set_path):\n",
    "    \"\"\"\n",
    "    Read the dataset and fill in NaN with empty string.\n",
    "    Transform the sentences into tokens\n",
    "    Fill in SID and token counts to calculate the error rate.\n",
    "    Return: a DataFrame filled in SID and token counts to calculate the error rate.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_excel(data_set_path)\n",
    "    data = data.fillna('')\n",
    "\n",
    "    arr_original = data['Original']\n",
    "    arr_transcribed = data['Sentence']\n",
    "\n",
    "    tokens_original = [nltk.word_tokenize(sent) for sent in arr_original]\n",
    "    tokens_transcribed = [nltk.word_tokenize(sent) for sent in arr_transcribed]\n",
    "\n",
    "    for i, (orig, transc) in enumerate(zip(tokens_original, tokens_transcribed)):\n",
    "        S, I, D = get_sid(orig, transc)\n",
    "        data.loc[i, 'S'] = S\n",
    "        data.loc[i, 'I'] = I\n",
    "        data.loc[i, 'D'] = D\n",
    "\n",
    "        data.loc[i, 'tokens_original'] = len(orig)\n",
    "        data.loc[i, 'tokens_transcribed'] = len(transc)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_wer(dataset):\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate (WER) between reference and hypothesis token lists.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens_original_total = sum(dataset['tokens_original'])\n",
    "    total_errors = sum(dataset['S']) + sum(dataset['I']) + sum(dataset['D'])\n",
    "    WER = total_errors / tokens_original_total\n",
    "\n",
    "    return WER, total_errors, tokens_original_total\n",
    "\n",
    "\n",
    "data_assembly = prepare_table_for_wer('assembly_WER.xlsx')\n",
    "WER, total_errors, tokens_original_total = get_wer(data_assembly)\n",
    "print(f'Total errors: {total_errors}')\n",
    "print(f'Total tokens in original: {tokens_original_total}')\n",
    "print(f'WER: {WER}')\n",
    "data_assembly.to_excel('assembly_WER_SID.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 40\n",
      "Total tokens in original: 1231.0\n",
      "WER: 0.03249390739236393\n"
     ]
    }
   ],
   "source": [
    "data_assembly = prepare_table_for_wer('whisper_WER.xlsx')\n",
    "WER, total_errors, tokens_original_total = get_wer(data_assembly)\n",
    "print(f'Total errors: {total_errors}')\n",
    "print(f'Total tokens in original: {tokens_original_total}')\n",
    "print(f'WER: {WER}')\n",
    "data_assembly.to_excel('whisper_WER_SID.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
