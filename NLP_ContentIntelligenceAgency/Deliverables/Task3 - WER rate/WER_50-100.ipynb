{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\artjo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 5\n",
      "Total tokens in original: 419.0\n",
      "WER: 0.011933174224343675\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def get_sid(reference_tokens, hypothesis_tokens):\n",
    "    \"\"\"\n",
    "    Calculate Substitutions (S), Insertions (I), and Deletions (D)\n",
    "    between reference and hypothesis token lists.\n",
    "    \"\"\"\n",
    "\n",
    "    matcher = Levenshtein.editops(reference_tokens, hypothesis_tokens)\n",
    "    \n",
    "    S, I, D = 0, 0, 0\n",
    "    for op in matcher:\n",
    "        if op[0] == 'replace':\n",
    "            S += 1\n",
    "        elif op[0] == 'insert':\n",
    "            I += 1\n",
    "        elif op[0] == 'delete':\n",
    "            D += 1\n",
    "    \n",
    "    return S, I, D\n",
    "\n",
    "def prepare_table_for_wer(data_set_path):\n",
    "    \"\"\"\n",
    "    Read the dataset and fill in NaN with empty string.\n",
    "    Transform the sentences into tokens\n",
    "    Fill in SID and token counts to calculate the error rate.\n",
    "    Return: a DataFrame filled in SID and token counts to calculate the error rate.\n",
    "    \"\"\"\n",
    "\n",
    "    data = pd.read_excel(data_set_path)\n",
    "    data = data.fillna('')\n",
    "\n",
    "    arr_original = data['Original']\n",
    "    arr_transcribed = data['Sentence']\n",
    "\n",
    "    tokens_original = [nltk.word_tokenize(sent) for sent in arr_original]\n",
    "    tokens_transcribed = [nltk.word_tokenize(sent) for sent in arr_transcribed]\n",
    "\n",
    "    for i, (orig, transc) in enumerate(zip(tokens_original, tokens_transcribed)):\n",
    "        S, I, D = get_sid(orig, transc)\n",
    "        data.loc[i, 'S'] = S\n",
    "        data.loc[i, 'I'] = I\n",
    "        data.loc[i, 'D'] = D\n",
    "\n",
    "        data.loc[i, 'tokens_original'] = len(orig)\n",
    "        data.loc[i, 'tokens_transcribed'] = len(transc)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def get_wer(dataset):\n",
    "    \"\"\"\n",
    "    Calculate Word Error Rate (WER) between reference and hypothesis token lists.\n",
    "    \"\"\"\n",
    "\n",
    "    tokens_original_total = sum(dataset['tokens_original'])\n",
    "    total_errors = sum(dataset['S']) + sum(dataset['I']) + sum(dataset['D'])\n",
    "    WER = total_errors / tokens_original_total\n",
    "\n",
    "    return WER, total_errors, tokens_original_total\n",
    "\n",
    "\n",
    "data_assembly = prepare_table_for_wer('assembly_WER_50-100_with_original.xlsx')\n",
    "WER, total_errors, tokens_original_total = get_wer(data_assembly)\n",
    "print(f'Total errors: {total_errors}')\n",
    "print(f'Total tokens in original: {tokens_original_total}')\n",
    "print(f'WER: {WER}')\n",
    "data_assembly.to_excel('assembly_WER_50-100_with_original_SID.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total errors: 11\n",
      "Total tokens in original: 414.0\n",
      "WER: 0.026570048309178744\n"
     ]
    }
   ],
   "source": [
    "data_assembly = prepare_table_for_wer('whisper_WER_50-100_with_original.xlsx')\n",
    "WER, total_errors, tokens_original_total = get_wer(data_assembly)\n",
    "print(f'Total errors: {total_errors}')\n",
    "print(f'Total tokens in original: {tokens_original_total}')\n",
    "print(f'WER: {WER}')\n",
    "data_assembly.to_excel('whisper_WER_50-100_with_original_SID.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "block_b",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
