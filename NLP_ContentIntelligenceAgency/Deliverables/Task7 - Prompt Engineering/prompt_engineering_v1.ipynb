{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1a156ee-1a9c-41a9-8f7a-695b1ea5064d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f040de-a67d-46de-b820-357c0af09c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== CONFIGURATION ====\n",
    "API_BASE = \"http://194.171.191.227:30080\"  # Server URL\n",
    "TOKEN = \"sk-ba75d292d92e457fa1615f217b11fe09\"  # Replace with your API token\n",
    "MODEL_NAME = \"llama3.2:3b\"  # Using a lightweight model for efficiency\n",
    "DATA_PATH = \"nlp_features_extra.xlsx\"  # Replace with your dataset path\n",
    "RESULTS_LOG = \"prompt_engineering_log.csv\"  # File to store prompt test results\n",
    "NUM_SAMPLES = 20  # Number of samples to test prompts on\n",
    "TARGET_EMOTIONS = [\"happiness\", \"sadness\", \"anger\", \"surprise\", \"fear\", \"disgust\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c77abe6-ff4c-4b39-a16b-92c8b42c075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== LOAD DATA ====\n",
    "# df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "\n",
    "# # Select a subset of data for evaluation\n",
    "# df_sample = df.sample(NUM_SAMPLES, random_state=42)\n",
    "\n",
    "# # Ensure dataset contains required columns\n",
    "# if \"ru_text\" not in df_sample.columns or \"general_emotion\" not in df_sample.columns:\n",
    "#     raise ValueError(\"Dataset must contain 'ru_text' and 'general_emotion' columns.\")\n",
    "\n",
    "# # Convert emotion labels to lowercase for consistency\n",
    "# df_sample[\"general_emotion\"] = df_sample[\"general_emotion\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "734670c3-ea36-414b-b71a-98c637f639e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "general_emotion\n",
      "fear         10\n",
      "neutral      10\n",
      "happiness    10\n",
      "sadness      10\n",
      "anger        10\n",
      "disgust      10\n",
      "surprise     10\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(DATA_PATH, engine=\"openpyxl\")\n",
    "\n",
    "# Suppose we want 10 examples of each emotion (fear, neutral, etc.)\n",
    "# We will create a dictionary that specifies how many samples we want per emotion.\n",
    "desired_counts = {\n",
    "    \"fear\": 10,\n",
    "    \"neutral\": 10,\n",
    "    \"happiness\": 10,\n",
    "    \"sadness\": 10,\n",
    "    \"anger\": 10,\n",
    "    \"disgust\": 10,\n",
    "    \"surprise\": 10\n",
    "}\n",
    "\n",
    "# Optional: we can store the subsets in a list and then concatenate them\n",
    "dfs_list = []\n",
    "\n",
    "for emotion, n_samples in desired_counts.items():\n",
    "    # Filter the dataframe for the given emotion\n",
    "    subset = df[df[\"general_emotion\"] == emotion]\n",
    "    \n",
    "    # If the subset has fewer rows than n_samples, sample(frac=1) \n",
    "    # will just give all rows for that emotion (or you can handle it differently)\n",
    "    if len(subset) >= n_samples:\n",
    "        chosen = subset.sample(n=n_samples, random_state=42)\n",
    "    else:\n",
    "        chosen = subset.sample(frac=1, random_state=42)  # or handle the case as you wish\n",
    "    dfs_list.append(chosen)\n",
    "\n",
    "# Concatenate all sampled subsets\n",
    "df_sample = pd.concat(dfs_list, ignore_index=True)\n",
    "\n",
    "# Now df_sample is your balanced subset with the desired distribution.\n",
    "# Double-check the distribution:\n",
    "print(df_sample[\"general_emotion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "474bbfd0-6efd-4735-8816-a7a43b690c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FUNCTION TO QUERY LLM ====\n",
    "# def query_llm(prompt):\n",
    "#     \"\"\" Sends a request to the locally deployed LLM and returns the model's response. \"\"\"\n",
    "#     url = f\"{API_BASE}/api/chat/completions\"\n",
    "#     headers = {\n",
    "#         \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "#         \"Content-Type\": \"application/json\"\n",
    "#     }\n",
    "#     data = {\n",
    "#         \"model\": MODEL_NAME,\n",
    "#         \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "#         \"temperature\": 0.0\n",
    "#     }\n",
    "    \n",
    "#     response = requests.post(url, headers=headers, json=data)\n",
    "    \n",
    "#     if response.status_code == 200:\n",
    "#         return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "#     else:\n",
    "#         print(f\"Error {response.status_code}: {response.text}\")\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "213cc30d-034c-4ec4-91e0-1f1a5834c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_llm_system(messages):\n",
    "    \"\"\"\n",
    "    Sends a request with both system and user messages.\n",
    "    \"\"\"\n",
    "    url = f\"{API_BASE}/api/chat/completions\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": 0.0,\n",
    "        \"top_p\": 0.9,\n",
    "        \"top_k\": 400,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"context_length\": 1000\n",
    "    }\n",
    "    response = requests.post(url, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29b12892-4b6a-4f10-811f-3b57d158e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FUNCTION TO EXTRACT EMOTION FROM TEXT ====\n",
    "def extract_emotion(text):\n",
    "    \"\"\"\n",
    "    Extracts the first occurrence of a target emotion or its synonym from the given text.\n",
    "    Uses regex to search for any of the target words.\n",
    "    If a synonym is found, it maps it to the corresponding target emotion.\n",
    "    Returns the extracted emotion if found, otherwise 'neutral'.\n",
    "    \"\"\"\n",
    "    # 1. Try to extract from JSON format\n",
    "    json_match = re.search(r'\\\"emotion\\\"\\s*:\\s*\\\"(.*?)\\\"', text, re.IGNORECASE)\n",
    "    if json_match:\n",
    "        emotion_found = json_match.group(1).lower()\n",
    "        if emotion_found in TARGET_EMOTIONS:\n",
    "            return emotion_found\n",
    "    \n",
    "    # 2. If not found, fallback to your synonyms mapping\n",
    "    # Define synonyms mapping (all in lowercase)\n",
    "    synonym_mapping = {\n",
    "        \"elation\": \"happiness\",\n",
    "        \"joy\": \"happiness\",\n",
    "        \"cheerfulness\": \"happiness\",\n",
    "        \"amusement\": \"happiness\",\n",
    "        \"admiration\": \"happiness\",\n",
    "        \"gratitude\": \"happiness\",\n",
    "        \"optimism\": \"happiness\",\n",
    "        \"pride\": \"happiness\",\n",
    "        \"excitement\": \"happiness\",\n",
    "        \"love\": \"happiness\",\n",
    "        \"relief\": \"happiness\",\n",
    "        \"caring\": \"happiness\",\n",
    "        \"approval\": \"happiness\",\n",
    "    \n",
    "        \"disappointment\": \"sadness\",\n",
    "        \"grief\": \"sadness\",\n",
    "        \"sadness\": \"sadness\",\n",
    "        \"remorse\": \"sadness\",\n",
    "        \"sorrow\": \"sadness\",\n",
    "        \"melancholy\": \"sadness\",\n",
    "    \n",
    "        \"annoyance\": \"anger\",\n",
    "        \"disapproval\": \"anger\",\n",
    "        \"anger\": \"anger\",\n",
    "    \n",
    "        \"surprise\": \"surprise\",\n",
    "        \"realization\": \"surprise\",\n",
    "        \"confusion\": \"surprise\",\n",
    "    \n",
    "        \"fear\": \"fear\",\n",
    "        \"nervousness\": \"fear\",\n",
    "    \n",
    "        \"disgust\": \"disgust\",\n",
    "        \"emberrasment\": \"disgust\",\n",
    "    \n",
    "        \"neutral\": \"neutral\"\n",
    "    }\n",
    "    # Combine target emotions and synonyms into one list\n",
    "    all_keywords = TARGET_EMOTIONS + list(synonym_mapping.keys())\n",
    "    # Create a regex pattern to match any keyword as a whole word, case-insensitive\n",
    "    pattern = r'\\b(' + '|'.join(all_keywords) + r')\\b'\n",
    "    # pattern = r\"classification is:\\s*(\\w+)\"\n",
    "    match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "    if match:\n",
    "        word = match.group(1).lower()\n",
    "        # Map synonym if necessary\n",
    "        return synonym_mapping.get(word, word)\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "634fdc8c-57ad-484b-8790-6b8f613a721f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== BASELINE PROMPT ====\n",
    "def baseline_prompt(sentence):\n",
    "    return f\"Analyze the following sentence and classify it as one of the six core emotions (happiness, sadness, anger, surprise, fear, disgust) or neutral:\\n\\n{sentence}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f02d4d5-c65a-457d-921d-cc9cde349ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== EXPERIMENTAL PROMPTS ====\n",
    "def few_shot_prompt(sentence):\n",
    "    \"\"\" Few-shot prompting: providing examples to improve accuracy. \"\"\"\n",
    "    return f\"\"\"\n",
    "Analyze the following sentence and classify it as one of the six core emotions (happiness, sadness, anger, surprise, fear, disgust) or neutral.\n",
    "\n",
    "Examples:\n",
    "1. \"I just won a million dollars!\" → happiness\n",
    "2. \"I can't believe he betrayed me.\" → anger\n",
    "3. \"She cried when she heard the news.\" → sadness\n",
    "4. \"This roller coaster is thrilling!\" → surprise\n",
    "\n",
    "Now classify the following sentence:\n",
    "\"{sentence}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def structured_prompt(sentence):\n",
    "    \"\"\" Structured instructions with format constraints. \"\"\"\n",
    "    return f\"\"\"\n",
    "Analyze the following sentence and determine the primary emotion expressed.\n",
    "- Available emotions: happiness, sadness, anger, surprise, fear, disgust, neutral\n",
    "- Return the answer in JSON format: {{ \"emotion\": \"emotion_name\" }}\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def definition_prompt(sentence):\n",
    "    \"\"\" Providing explicit definitions for emotions. \"\"\"\n",
    "    return f\"\"\"\n",
    "Analyze the following sentence and classify it based on the provided definitions.\n",
    "\n",
    "Definitions:\n",
    "- Happiness: A positive, joyful feeling\n",
    "- Sadness: A feeling of loss or disappointment\n",
    "- Anger: A strong feeling of displeasure\n",
    "- Surprise: A reaction to something unexpected\n",
    "- Fear: A response to perceived danger\n",
    "- Disgust: A feeling of strong dislike\n",
    "- Neutral: No strong emotional reaction\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\n",
    "Which category best describes this sentence?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d39145a1-376b-4287-b8b3-14e0a86fecc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refined_structured_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Structured instructions with more explicit directions and examples.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an assistant specialized in emotion analysis. \n",
    "Your task is to classify the following sentence into exactly one of these categories:\n",
    "[happiness, sadness, anger, surprise, fear, disgust, neutral].\n",
    "\n",
    "Instructions:\n",
    "1. Read the sentence carefully.\n",
    "2. Decide which single emotion from the list best fits the sentence.\n",
    "3. Return ONLY the final emotion in JSON format, like: {{ \"emotion\": \"anger\" }}\n",
    "\n",
    "Here is the sentence:\n",
    "\\\"{sentence}\\\"\n",
    "\n",
    "If you are not sure which category fits best, choose \"neutral\".\n",
    "\"\"\"\n",
    "\n",
    "def strict_format_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Prompt that forces the model to respond in a strict JSON format.\n",
    "    \"\"\"\n",
    "    return f\"\"\"\n",
    "You are an advanced AI for emotion classification. \n",
    "Classify the sentence into exactly one of these emotions:\n",
    "[happiness, sadness, anger, surprise, fear, disgust, neutral].\n",
    "\n",
    "RETURN ONLY IN THIS EXACT FORMAT (no extra text):\n",
    "{{\"emotion\":\"VALUE\"}}\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\"\"\"\n",
    "\n",
    "def improved_few_shot_prompt(sentence):\n",
    "    return f\"\"\"\n",
    "You are an AI for emotion classification. Classify each sentence into \n",
    "[happiness, sadness, anger, surprise, fear, disgust, neutral].\n",
    "\n",
    "Examples:\n",
    "1. \"I just won a million dollars!\" => happiness\n",
    "2. \"I hate it when you yell at me.\" => anger\n",
    "3. \"I feel so lonely without you.\" => sadness\n",
    "4. \"This roller coaster is thrilling!\" => surprise\n",
    "5. \"I find rotten food gross.\" => disgust\n",
    "6. \"I feel anxious before the exam.\" => fear\n",
    "7. \"I'm doing okay, nothing special.\" => neutral\n",
    "\n",
    "Now classify this sentence: \"{sentence}\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d05ace36-74f1-494d-927f-1d1ff045e7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def system_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Prompt that uses a system message to strongly guide the model.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": (\n",
    "            \"You are a sophisticated emotion classification AI. \"\n",
    "            \"You will respond ONLY with one of these categories: \"\n",
    "            \"[happiness, sadness, anger, surprise, fear, disgust, neutral].\"\n",
    "            \"No additional text or explanation.\"\n",
    "        )},\n",
    "        {\"role\": \"user\", \"content\": f\"Classify the emotion of this sentence:\\n{sentence}\"}\n",
    "    ]\n",
    "    return messages\n",
    "\n",
    "def system_few_shot_prompt(sentence):\n",
    "    \"\"\"\n",
    "    Combines a system instruction with a few-shot approach.\n",
    "    \"\"\"\n",
    "    system_content = (\n",
    "        \"You are a sophisticated emotion classification AI. \"\n",
    "        \"You will respond ONLY with the single emotion from this list: \"\n",
    "        \"[happiness, sadness, anger, surprise, fear, disgust, neutral]. \"\n",
    "        \"If the sentence does not clearly express any emotion, choose 'neutral'. \"\n",
    "        \"No additional text or explanation.\"\n",
    "    )\n",
    "    \n",
    "    examples_text = (\n",
    "        \"Here are examples:\\n\"\n",
    "        \"1. 'good bot' => happiness\\n\"\n",
    "        \"1. 'Enjoy your dinner' => happiness\\n\"\n",
    "        \"1. 'it doesn t count if you re drunk ? Wow.. Imagine that being used as an excuse in other situations.' => happiness\\n\"\n",
    "        \n",
    "        \"2. 'I hate it when you yell at me.' => anger\\n\"\n",
    "        \"2. 'I hate it when you yell at me.' => anger\\n\"\n",
    "        \"2. 'I hate it when you yell at me.' => anger\\n\"\n",
    "        \n",
    "        \"3. 'I'm so lonely without you.' => sadness\\n\"\n",
    "        \"4. 'This roller coaster is thrilling!' => surprise\\n\"\n",
    "        \"5. 'I find rotten food gross.' => disgust\\n\"\n",
    "        \"6. 'I feel anxious before the exam.' => fear\\n\"\n",
    "        \"7. 'I'm doing okay, nothing special.' => neutral\\n\"\n",
    "        \"8. 'This is not terrible, but not great either.' => neutral\\n\"\n",
    "    )\n",
    "    \n",
    "    user_prompt = (\n",
    "        f\"Classify the emotion of this sentence:\\n{sentence}\\n\\n\"\n",
    "        \"Return only one emotion from the list above.\"\n",
    "    )\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_content},\n",
    "        {\"role\": \"user\", \"content\": examples_text},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "246d7b22-e4c6-4656-8eb0-773d3f976e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== FUNCTION TO TEST PROMPTS ====\n",
    "def test_prompt(prompt_function, prompt_name):\n",
    "    \"\"\" \n",
    "    Runs a test on a given prompt format and logs results.\n",
    "    It queries the LLM, extracts the emotion keyword from the response, \n",
    "    and compares it to the true label.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    print(f\"\\nRunning test for: {prompt_name}\")\n",
    "    \n",
    "    for _, row in df_sample.iterrows():\n",
    "        sentence = row[\"text\"]\n",
    "        true_label = row[\"general_emotion\"]\n",
    "        \n",
    "        # Generate prompt and query the LLM\n",
    "        prompt = prompt_function(sentence)\n",
    "        # predicted_label = query_llm(prompt)\n",
    "        predicted_label = query_llm_system(prompt)\n",
    "        \n",
    "        # Post-process output (convert to lowercase, strip spaces)\n",
    "        if predicted_label:\n",
    "            predicted_label = predicted_label.lower().strip()\n",
    "            # Extract emotion keyword from the response using regex mapping\n",
    "            predicted_label = extract_emotion(predicted_label)\n",
    "        else:\n",
    "            predicted_label = \"neutral\"\n",
    "        \n",
    "        # Ensure only valid labels are considered\n",
    "        if predicted_label not in TARGET_EMOTIONS:\n",
    "            predicted_label = \"neutral\"  # Default to neutral if unclear\n",
    "        \n",
    "        predictions.append((sentence, true_label, predicted_label))\n",
    "    \n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(predictions, columns=[\"Sentence\", \"True_Label\", \"Predicted_Label\"])\n",
    "    \n",
    "    # Compute evaluation metrics\n",
    "    f1 = f1_score(results_df[\"True_Label\"], results_df[\"Predicted_Label\"], average=\"macro\")\n",
    "    unique_labels = sorted(results_df[\"True_Label\"].unique())  # Get unique true labels from the sample\n",
    "    report = classification_report(\n",
    "        results_df[\"True_Label\"], \n",
    "        results_df[\"Predicted_Label\"], \n",
    "        labels=unique_labels,            # Specify labels present in the sample\n",
    "        target_names=unique_labels,      # Use the same order for target_names\n",
    "        zero_division=0\n",
    "    )\n",
    "    conf_matrix = confusion_matrix(results_df[\"True_Label\"], results_df[\"Predicted_Label\"], labels=unique_labels)\n",
    "    \n",
    "    print(f\"\\n=== Results for {prompt_name} ===\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    # Save results to log file\n",
    "    results_df[\"Prompt_Type\"] = prompt_name\n",
    "    results_df.to_csv(RESULTS_LOG, mode=\"a\", header=not pd.io.common.file_exists(RESULTS_LOG), index=False)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13926f24-b80e-4a82-9953-e9d836bd3cee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running test for: system_few_shot_prompt\n",
      "\n",
      "=== Results for system_few_shot_prompt ===\n",
      "F1 Score: 0.4981\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.40      0.40      0.40        10\n",
      "     disgust       0.44      0.80      0.57        10\n",
      "        fear       1.00      0.50      0.67        10\n",
      "   happiness       0.53      0.80      0.64        10\n",
      "     neutral       0.50      0.10      0.17        10\n",
      "     sadness       0.83      0.50      0.62        10\n",
      "    surprise       0.36      0.50      0.42        10\n",
      "\n",
      "    accuracy                           0.51        70\n",
      "   macro avg       0.58      0.51      0.50        70\n",
      "weighted avg       0.58      0.51      0.50        70\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==== RUN EXPERIMENTS ====\n",
    "# f1_baseline = test_prompt(baseline_prompt, \"Baseline\") #58(2) 42(5)\n",
    "# f1_few_shot = test_prompt(few_shot_prompt, \"Few-Shot\") #54\n",
    "# f1_structured = test_prompt(structured_prompt, \"Structured\") #47\n",
    "# f1_definition = test_prompt(definition_prompt, \"Definition-Based\") #30\n",
    "# f1_refined_structured = test_prompt(refined_structured_prompt, \"refined_structured_prompt\") #61(2) 52(5)\n",
    "# f1_strict_format = test_prompt(strict_format_prompt, \"strict_format_prompt\") #67(2) 56(5)\n",
    "# f1_improved_few_shot = test_prompt(improved_few_shot_prompt, \"improved_few_shot_prompt\") #55(2) 50(5)\n",
    "\n",
    "# f1_system = test_prompt(system_prompt, \"system_prompt\")\n",
    "f1_system_few_shot = test_prompt(system_few_shot_prompt, \"system_few_shot_prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f9b6576-7a21-4a7d-9efa-8e3b207318ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "closing parenthesis ']' does not match opening parenthesis '(' on line 2 (2848820042.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    (\"strict_format\", f1_strict_format)],\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m closing parenthesis ']' does not match opening parenthesis '(' on line 2\n"
     ]
    }
   ],
   "source": [
    "# ==== SELECT BEST PROMPT ====\n",
    "best_prompt = max(\n",
    "    [(\"Baseline\", f1_baseline),\n",
    "     # (\"Few-Shot\", f1_few_shot),\n",
    "     # (\"Structured\", f1_structured),\n",
    "     # (\"Definition-Based\", f1_definition)],\n",
    "     (\"refined_structured\", f1_refined_structured)],\n",
    "     (\"strict_format\", f1_strict_format)],\n",
    "     (\"improved_few_shot\", f1_improved_few_shot)],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Best prompt: {best_prompt[0]} with F1-score: {best_prompt[1]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
