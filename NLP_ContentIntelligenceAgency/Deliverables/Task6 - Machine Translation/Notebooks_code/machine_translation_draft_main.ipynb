{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining data from OPUS for model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–∏ –ª–æ–∫–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏)\n",
    "ru_file = \"TED2020.en-ru.ru\"\n",
    "en_file = \"TED2020.en-ru.en\"\n",
    "output_file = \"TED2020_translations.xlsx\"\n",
    "\n",
    "# –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª—ã –ø–æ—Å—Ç—Ä–æ—á–Ω–æ\n",
    "with open(ru_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    ru_sentences = f.readlines()\n",
    "\n",
    "with open(en_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    en_sentences = f.readlines()\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ —Å–æ–≤–ø–∞–¥–∞–µ—Ç\n",
    "if len(ru_sentences) != len(en_sentences):\n",
    "    print(\"–û—à–∏–±–∫–∞: –§–∞–π–ª—ã —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫!\")\n",
    "else:\n",
    "    print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ru_sentences)} –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\")\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º DataFrame\n",
    "df = pd.DataFrame({\"Russian\": [s.strip() for s in ru_sentences], \n",
    "                   \"English\": [s.strip() for s in en_sentences]})\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Excel\n",
    "df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"üéâ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª {output_file}\")\"\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared and saved: train(312011), val(39001), test(39002)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"TED2020_translations.xlsx\")\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "train, temp = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Save datasets as Excel files\n",
    "train.to_excel(\"train.xlsx\", index=False, engine=\"openpyxl\")\n",
    "val.to_excel(\"val.xlsx\", index=False, engine=\"openpyxl\")\n",
    "test.to_excel(\"test.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"Data prepared and saved: train({len(train)}), val({len(val)}), test({len(test)})\")\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data cleaned and saved as train_cleaned.xlsx & val_cleaned.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load training and validation data from Excel\n",
    "train_data = pd.read_excel(\"train.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Drop NaN values and convert everything to strings\n",
    "train_data = train_data.dropna().astype(str)\n",
    "val_data = val_data.dropna().astype(str)\n",
    "\n",
    "# Save cleaned data (optional step for debugging)\n",
    "train_data.to_excel(\"train_cleaned.xlsx\", index=False, engine=\"openpyxl\")\n",
    "val_data.to_excel(\"val_cleaned.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Data cleaned and saved as train_cleaned.xlsx & val_cleaned.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the MarianMT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.3)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch)\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch)\n",
      "  Downloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m85.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading pyarrow-19.0.1-cp311-cp311-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m68.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (133 kB)\n",
      "Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
      "Installing collected packages: triton, sentencepiece, nvidia-cusparselt-cu12, mpmath, xxhash, sympy, safetensors, regex, pyarrow, propcache, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, multidict, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, huggingface-hub, aiosignal, tokenizers, nvidia-cusolver-cu12, aiohttp, transformers, torch, datasets\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 datasets-3.4.1 dill-0.3.8 filelock-3.18.0 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.29.3 mpmath-1.3.0 multidict-6.2.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 propcache-0.3.0 pyarrow-19.0.1 regex-2024.11.6 safetensors-0.5.3 sentencepiece-0.2.0 sympy-1.13.1 tokenizers-0.21.1 torch-2.6.0 transformers-4.49.0 triton-3.2.0 xxhash-3.5.0 yarl-1.18.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers[torch] accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Small dataset for fast training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prepared smaller dataset with 8000 training and 2000 validation samples.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_excel(\"train_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Take only a subset of data (e.g., 10,000 samples)\n",
    "subset_size = 10000\n",
    "df = df.sample(n=subset_size, random_state=42)  # Randomly select 10,000 rows\n",
    "\n",
    "# Split into train/validation\n",
    "train, val = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the smaller dataset\n",
    "train.to_excel(\"train_small.xlsx\", index=False, engine=\"openpyxl\")\n",
    "val.to_excel(\"val_small.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"‚úÖ Prepared smaller dataset with {len(train)} training and {len(val)} validation samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second try training with small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 19:44:58.328579: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-18 19:44:58.341621: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742327098.356305   18486 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742327098.360866   18486 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742327098.373535   18486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742327098.373551   18486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742327098.373553   18486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742327098.373555   18486 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-18 19:44:58.377663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO: PyTorch version 2.6.0 available.\n",
      "INFO: TensorFlow version 2.19.0 available.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c25531790274766b9b1334d8816de52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f817087af57142628a1654dc4c539dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 01:16, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.795100</td>\n",
       "      <td>0.730192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.691100</td>\n",
       "      <td>0.678546</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ Training completed! Model saved in 'custom_translation_model'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the smaller dataset\n",
    "train_data = pd.read_excel(\"train_small.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_small.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Load pre-trained MarianMT model\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments (optimized for faster training)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_translation_model\",  # ‚úÖ Save model in a clear directory\n",
    "    num_train_epochs=2,  # ‚úÖ Reduce epochs for faster training\n",
    "    per_device_train_batch_size=16,  # ‚úÖ Increase batch size to speed up training\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1  # ‚úÖ Keep only the latest checkpoint\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model in a clear directory\n",
    "model.save_pretrained(\"custom_translation_model\")\n",
    "tokenizer.save_pretrained(\"custom_translation_model\")\n",
    "\n",
    "print(\"üéâ Training completed! Model saved in 'custom_translation_model'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second try, others hyperparametrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc9428d8ef5434b9a0812384cc9e6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f626ccda0e1047c5904e7d8fcc5074df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 01:10, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.167600</td>\n",
       "      <td>1.126219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.112500</td>\n",
       "      <td>0.946095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.921500</td>\n",
       "      <td>0.869714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Model saved in 'custom_translation_model_v2'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the smaller dataset\n",
    "train_data = pd.read_excel(\"train_small.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_small.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Load pre-trained MarianMT model\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Optimized Training Arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_translation_model_v2\",  # ‚úÖ New folder for improved model\n",
    "    num_train_epochs=4,  # ‚úÖ More epochs for better training\n",
    "    per_device_train_batch_size=32,  # ‚úÖ Larger batch size (if enough memory)\n",
    "    gradient_accumulation_steps=4,  # ‚úÖ Effective larger batch size if memory is limited\n",
    "    learning_rate=3e-5,  # ‚úÖ Slower learning rate for stability\n",
    "    weight_decay=0.01,  # ‚úÖ Regularization to prevent overfitting\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "model.save_pretrained(\"custom_translation_model_v2\")\n",
    "tokenizer.save_pretrained(\"custom_translation_model_v2\")\n",
    "\n",
    "print(\"Training completed! Model saved in 'custom_translation_model_v2'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Third model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b06d8a7cbbc44188bc04a1329cc464e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f3f469386aa4cc3806f829ecbb0dfd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 01:35, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.315128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.291100</td>\n",
       "      <td>0.315201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.277000</td>\n",
       "      <td>0.316235</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Improved model (v1.5) training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load dataset\n",
    "train_data = pd.read_excel(\"train_small.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_small.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing function with label smoothing support\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_translation_model_v1_5\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True  # Required for generation-based eval\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save model\n",
    "model.save_pretrained(\"custom_translation_model_v1_5\")\n",
    "tokenizer.save_pretrained(\"custom_translation_model_v1_5\")\n",
    "\n",
    "print(\"‚úÖ Improved model (v1.5) training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### With EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bbc98286bc4670a9beaf89b4459476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3970: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8515b4664f374a44915ae6b1a4e297fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='2500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 750/2500 01:33 < 03:39, 7.99 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321400</td>\n",
       "      <td>0.315831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.289000</td>\n",
       "      <td>0.317355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.320317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training complete! Best model saved in 'custom_translation_model_v1_5_earlystop'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import (\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# üìå Step 1: Load the dataset\n",
    "train_data = pd.read_excel(\"train_small.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_small.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# üìå Step 2: Load model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# üìå Step 3: Preprocessing\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# üìå Step 4: Training arguments with EarlyStopping setup\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_translation_model_v1_5_earlystop\",\n",
    "    num_train_epochs=10,  # max limit, early stopping will stop earlier if needed\n",
    "    per_device_train_batch_size=32,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,  # ‚¨ÖÔ∏è restores best model\n",
    "    metric_for_best_model=\"eval_loss\",  # based on validation loss\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# üìå Step 5: Trainer with EarlyStoppingCallback\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  # ‚èπÔ∏è stop after 2 non-improving epochs\n",
    ")\n",
    "\n",
    "# üìå Step 6: Train and save\n",
    "trainer.train()\n",
    "\n",
    "model.save_pretrained(\"custom_translation_model_v1_5_earlystop\")\n",
    "tokenizer.save_pretrained(\"custom_translation_model_v1_5_earlystop\")\n",
    "\n",
    "print(\"‚úÖ Training complete! Best model saved in 'custom_translation_model_v1_5_earlystop'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ 'translated_assembly_WER_v1_5.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "file_path = \"assembly_WER.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "# üîπ –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞\n",
    "if \"Sentence\" not in df.columns:\n",
    "    raise ValueError(\"‚ùå –í —Ñ–∞–π–ª–µ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'Sentence'. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞!\")\n",
    "\n",
    "# üîπ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å v1_5_earlystop\n",
    "model_path = \"custom_translation_model_v1_5_earlystop\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "\n",
    "# üîπ –ò—Å–ø–æ–ª—å–∑—É–µ–º GPU –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# üîπ –§—É–Ω–∫—Ü–∏—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞\n",
    "def batch_translate(sentences, model, tokenizer):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "# üîπ –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –±–∞—Ç—á–∏\n",
    "batch_size = 16\n",
    "sentences = df[\"Sentence\"].astype(str).tolist()\n",
    "translations = [\n",
    "    translation\n",
    "    for i in range(0, len(sentences), batch_size)\n",
    "    for translation in batch_translate(sentences[i:i+batch_size], model, tokenizer)\n",
    "]\n",
    "\n",
    "# üîπ –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "df[\"Translation_v1_5\"] = translations\n",
    "df.to_excel(\"translated_assembly_WER_v1_5.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω –∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ 'translated_assembly_WER_v1_5.xlsx'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ff2027192e48b585c5c25038785636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad5b748630b74d9db2b16edf9d406d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 04:33, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.254700</td>\n",
       "      <td>1.032827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.930700</td>\n",
       "      <td>0.867239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.852400</td>\n",
       "      <td>0.793226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.785000</td>\n",
       "      <td>0.750331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.736400</td>\n",
       "      <td>0.723534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.700200</td>\n",
       "      <td>0.705162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.690700</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.671500</td>\n",
       "      <td>0.684287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.680194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.643400</td>\n",
       "      <td>0.678575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'custom_translation_model_final'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "\n",
    "# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "train_data = pd.read_excel(\"train_small.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_small.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Dataset —Ñ–æ—Ä–º–∞—Ç–∞ Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å MarianMT (Transformer)\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# ‚úÖ –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_translation_model_final\",  # üèÅ –§–∏–Ω–∞–ª—å–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "    num_train_epochs=10,  # üî• –î–æ–ª—å—à–µ –æ–±—É—á–µ–Ω–∏–µ\n",
    "    per_device_train_batch_size=16,  # ‚öñÔ∏è –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "    gradient_accumulation_steps=4,  # üß† –ù—É–∂–µ–Ω, –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –º–∞–ª–æ\n",
    "    learning_rate=2e-5,  # üöÄ –°—Ç–∞–±–∏–ª—å–Ω—ã–π –ª–µ—Ä–Ω–∏–Ω–≥ —Ä–µ–π—Ç\n",
    "    weight_decay=0.01,  # üõ°Ô∏è –ó–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,  # üóëÔ∏è –•—Ä–∞–Ω–∏–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–π–Ω—Ç\n",
    "    fp16=True,  # ‚ö° –ò—Å–ø–æ–ª—å–∑—É–µ–º Mixed Precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "    load_best_model_at_end=True,  # üèÜ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")\n",
    "\n",
    "# ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º Early Stopping (–µ—Å–ª–∏ loss –Ω–µ –ø–∞–¥–∞–µ—Ç 2 —ç–ø–æ—Ö–∏ ‚Äî —Å—Ç–æ–ø)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "model.save_pretrained(\"custom_translation_model_final\")\n",
    "tokenizer.save_pretrained(\"custom_translation_model_final\")\n",
    "\n",
    "print(\"‚úÖ –§–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'custom_translation_model_final'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417dbc6221a24d4ea5633ee1ceb82fae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ee03134e6944e6b99532a982e756d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [625/625 02:16, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.263600</td>\n",
       "      <td>1.046986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.954200</td>\n",
       "      <td>0.892963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.831931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.841900</td>\n",
       "      <td>0.803147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.811500</td>\n",
       "      <td>0.794744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'custom_translation_model_v3'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "\n",
    "# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "train_data = pd.read_excel(\"train_small.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_small.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Dataset —Ñ–æ—Ä–º–∞—Ç–∞ Hugging Face\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# ‚úÖ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å MarianMT —Å –Ω—É–ª—è (–±–µ–∑ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö —á–µ–∫–ø–æ–π–Ω—Ç–æ–≤)\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# ‚úÖ –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# ‚úÖ –ü—Ä–∏–º–µ–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—é\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ‚úÖ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_translation_model_v3\",  # –ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "    num_train_epochs=5,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "    per_device_train_batch_size=16,  # –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞\n",
    "    gradient_accumulation_steps=4,  # –ù–∞–∫–æ–ø–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç)\n",
    "    learning_rate=2e-5,  # –ú–µ–Ω—å—à–∏–π –ª–µ—Ä–Ω–∏–Ω–≥ —Ä–µ–π—Ç –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "    weight_decay=0.01,  # –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
    "    evaluation_strategy=\"epoch\",  # –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥—ã–π —ç–ø–æ—Ö\n",
    "    save_strategy=\"epoch\",  # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∫–∞–∂–¥—ã–π —ç–ø–æ—Ö\n",
    "    logging_steps=50,  # –õ–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥—É—é 50-—é –∏—Ç–µ—Ä–∞—Ü–∏—é\n",
    "    save_total_limit=1,  # –•—Ä–∞–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–µ–∫–ø–æ–π–Ω—Ç\n",
    "    fp16=True,  # –ò—Å–ø–æ–ª—å–∑—É–µ–º mixed precision –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è\n",
    "    load_best_model_at_end=True,  # –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à–∏–π —á–µ–∫–ø–æ–π–Ω—Ç –ø–æ –≤–∞–ª–∏–¥. –ø–æ—Ç–µ—Ä–µ\n",
    "    metric_for_best_model=\"eval_loss\",  # –ú–µ—Ç—Ä–∏–∫–∞ –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
    "    greater_is_better=False,  # –ß–µ–º –º–µ–Ω—å—à–µ eval_loss, —Ç–µ–º –ª—É—á—à–µ\n",
    ")\n",
    "\n",
    "# ‚úÖ –î–æ–±–∞–≤–ª—è–µ–º Early Stopping (–µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ —É–ª—É—á—à–∏–ª–∞—Å—å 2 —ç–ø–æ—Ö–∏ ‚Äî –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è —Å –Ω—É–ª—è\n",
    "trainer.train()\n",
    "\n",
    "# ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è\n",
    "model.save_pretrained(\"custom_translation_model_v3\")\n",
    "tokenizer.save_pretrained(\"custom_translation_model_v3\")\n",
    "\n",
    "print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'custom_translation_model_v3'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'translated_assembly_WER.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "file_path = \"assembly_WER.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞\n",
    "if \"Sentence\" not in df.columns:\n",
    "    raise ValueError(\"‚ùå –í —Ñ–∞–π–ª–µ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'Sentence'. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞!\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–µ –º–æ–¥–µ–ª–∏\n",
    "model1_name = \"custom_translation_model\"\n",
    "model2_name = \"custom_translation_model_v2\"\n",
    "\n",
    "tokenizer1 = MarianTokenizer.from_pretrained(model1_name)\n",
    "tokenizer2 = MarianTokenizer.from_pretrained(model2_name)\n",
    "\n",
    "model1 = MarianMTModel.from_pretrained(model1_name)\n",
    "model2 = MarianMTModel.from_pretrained(model2_name)\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model1.to(device)\n",
    "model2.to(device)\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞\n",
    "def batch_translate(sentences, model, tokenizer):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "# –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ batch-–∏ –ø–æ 16 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "batch_size = 16\n",
    "\n",
    "df[\"Translation_Model_1\"] = [\n",
    "    trans for i in range(0, len(df), batch_size)\n",
    "    for trans in batch_translate(df[\"Sentence\"].astype(str).tolist()[i:i + batch_size], model1, tokenizer1)\n",
    "]\n",
    "\n",
    "df[\"Translation_Model_2\"] = [\n",
    "    trans for i in range(0, len(df), batch_size)\n",
    "    for trans in batch_translate(df[\"Sentence\"].astype(str).tolist()[i:i + batch_size], model2, tokenizer2)\n",
    "]\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –Ω–æ–≤—ã–π Excel-—Ñ–∞–π–ª\n",
    "output_file = \"translated_assembly_WER.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ 'translated_assembly_WER.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "file_path = \"assembly_WER.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞\n",
    "if \"Sentence\" not in df.columns:\n",
    "    raise ValueError(\"‚ùå –í —Ñ–∞–π–ª–µ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'Sentence'. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ñ–∞–π–ª–∞!\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–µ –º–æ–¥–µ–ª–∏\n",
    "model1_name = \"custom_translation_model\"\n",
    "model2_name = \"custom_translation_model_v2\"\n",
    "\n",
    "tokenizer1 = MarianTokenizer.from_pretrained(model1_name)\n",
    "tokenizer2 = MarianTokenizer.from_pretrained(model2_name)\n",
    "\n",
    "model1 = MarianMTModel.from_pretrained(model1_name).to(\"cuda\")  # ‚úÖ –°—Ä–∞–∑—É –Ω–∞ GPU\n",
    "model2 = MarianMTModel.from_pretrained(model2_name).to(\"cuda\")  # ‚úÖ –°—Ä–∞–∑—É –Ω–∞ GPU\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞\n",
    "def batch_translate(sentences, model, tokenizer):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(\"cuda\")\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "# –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ batch-–∏ –ø–æ 16 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "batch_size = 16\n",
    "\n",
    "df[\"Translation_Model_1\"] = [\n",
    "    trans for i in range(0, len(df), batch_size)\n",
    "    for trans in batch_translate(df[\"Sentence\"].astype(str).tolist()[i:i + batch_size], model1, tokenizer1)\n",
    "]\n",
    "\n",
    "df[\"Translation_Model_2\"] = [\n",
    "    trans for i in range(0, len(df), batch_size)\n",
    "    for trans in batch_translate(df[\"Sentence\"].astype(str).tolist()[i:i + batch_size], model2, tokenizer2)\n",
    "]\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –Ω–æ–≤—ã–π Excel-—Ñ–∞–π–ª\n",
    "output_file = \"translated_assembly_WER.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"‚úÖ –ü–µ—Ä–µ–≤–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω! –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First try of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 16:31:51.257754: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-18 16:31:51.270838: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742315511.285192   15960 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742315511.289643   15960 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742315511.302109   15960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742315511.302125   15960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742315511.302127   15960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742315511.302129   15960 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-18 16:31:51.306147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "INFO: PyTorch version 2.6.0 available.\n",
      "INFO: TensorFlow version 2.19.0 available.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64bd98b740d4ba58bd30d9424378a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/309030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900b3f5c6d4c408ab870f87fe69026f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='115887' max='115887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [115887/115887 1:21:47, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.443300</td>\n",
       "      <td>0.363478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.367000</td>\n",
       "      <td>0.337283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.327943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Model saved\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load cleaned training and validation data from Excel\n",
    "train_data = pd.read_excel(\"train_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Convert Pandas DataFrame to Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Load the MarianMT model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Preprocessing function for tokenization\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization with batch processing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\".\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save trained model\n",
    "model.save_pretrained(\".\")\n",
    "tokenizer.save_pretrained(\".\")\n",
    "\n",
    "print(\"Training completed! Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINAL TRAINING - !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: PyTorch version 2.6.0 available.\n",
      "INFO: TensorFlow version 2.19.0 available.\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017f0a830a7f4ae8b70cbf850476e904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/309030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8490bb404cf14fa78ab33ddd8db583f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='48280' max='48280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [48280/48280 2:26:31, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.472300</td>\n",
       "      <td>0.432152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.420500</td>\n",
       "      <td>0.390194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.410700</td>\n",
       "      <td>0.371440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.384200</td>\n",
       "      <td>0.361543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.365300</td>\n",
       "      <td>0.354061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.354600</td>\n",
       "      <td>0.349739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.359700</td>\n",
       "      <td>0.345944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.341400</td>\n",
       "      <td>0.343992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.342949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.332500</td>\n",
       "      <td>0.342440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 6, 'bad_words_ids': [[62517]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed. Model saved to 'custom_translation_model_full'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import MarianMTModel, MarianTokenizer, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the full training and validation data\n",
    "train_data = pd.read_excel(\"train_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Load the MarianMT model and tokenizer\n",
    "model_name = \"Helsinki-NLP/opus-mt-ru-en\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization to datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"custom_translation_model_full\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    logging_steps=50,\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Create the trainer with EarlyStopping\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Save the final model\n",
    "model.save_pretrained(\"custom_translation_model_full\")\n",
    "tokenizer.save_pretrained(\"custom_translation_model_full\")\n",
    "\n",
    "print(\"Training completed. Model saved to 'custom_translation_model_full'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed successfully. Output saved to 'translated_assembly_WER_full.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the input dataset\n",
    "file_path = \"assembly_WER.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "# Load the final trained translation model\n",
    "model_path = \"custom_translation_model_full\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Batch translation function\n",
    "def batch_translate(sentences, model, tokenizer):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    translated = model.generate(**inputs)\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "# Translate in batches\n",
    "batch_size = 16\n",
    "sentences = df[\"Sentence\"].astype(str).tolist()\n",
    "translations = [\n",
    "    translation\n",
    "    for i in range(0, len(sentences), batch_size)\n",
    "    for translation in batch_translate(sentences[i:i + batch_size], model, tokenizer)\n",
    "]\n",
    "\n",
    "# Save the translations\n",
    "df[\"Translation\"] = translations\n",
    "df.to_excel(\"translated_assembly_WER_full.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Translation completed successfully. Output saved to 'translated_assembly_WER_full.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Translation with beam=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed using beam search. Output saved to 'translated_assembly_WER_beam5.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Load the input Excel file\n",
    "file_path = \"assembly_WER.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "# Ensure the sentence column exists\n",
    "if \"Sentence\" not in df.columns:\n",
    "    raise ValueError(\"The file must contain a column named 'Sentence'.\")\n",
    "\n",
    "# Load your trained translation model\n",
    "model_path = \"custom_translation_model_full\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_path)\n",
    "model = MarianMTModel.from_pretrained(model_path)\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Translation function with beam search\n",
    "def batch_translate(sentences, model, tokenizer, num_beams=5):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    translated = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "# Translate in batches\n",
    "batch_size = 16\n",
    "sentences = df[\"Sentence\"].astype(str).tolist()\n",
    "translations = [\n",
    "    translation\n",
    "    for i in range(0, len(sentences), batch_size)\n",
    "    for translation in batch_translate(sentences[i:i + batch_size], model, tokenizer, num_beams=5)\n",
    "]\n",
    "\n",
    "# Save results to new column and file\n",
    "df[\"Translation_beam5\"] = translations\n",
    "df.to_excel(\"translated_assembly_WER_beam5.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Translation completed using beam search. Output saved to 'translated_assembly_WER_beam5.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training with mBART with small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985a77d82b31435a9210c71f7c571e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5850b1bffb5e452f9b1016957ee55103",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_72207/2351858815.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 07:40, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.461100</td>\n",
       "      <td>2.448836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.372900</td>\n",
       "      <td>2.400036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.309700</td>\n",
       "      <td>2.417953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.257800</td>\n",
       "      <td>2.405698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2810: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mBART training on small dataset completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the small train and validation sets\n",
    "train_data = pd.read_excel(\"train_small.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_small.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Load mBART model and tokenizer\n",
    "model_name = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Set source and target language\n",
    "tokenizer.src_lang = \"ru_RU\"\n",
    "target_lang = \"en_XX\"\n",
    "\n",
    "# Tokenization function\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples[\"Russian\"], max_length=128, truncation=True, padding=\"max_length\")\n",
    "    targets = tokenizer(examples[\"English\"], max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "# Apply tokenization\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Training settings\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mbart_translation_tuned\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=4,  # –Ω–æ –±—É–¥–µ—Ç —Ä–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "    label_smoothing_factor=0.1,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"mbart_translation_tuned\")\n",
    "tokenizer.save_pretrained(\"mbart_translation_tuned\")\n",
    "\n",
    "print(\"mBART training on small dataset completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Translaiting with small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed using mBART. Output saved to 'translated_assembly_WER_mbart.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# Load the test file\n",
    "file_path = \"assembly_WER.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "# Ensure the sentence column exists\n",
    "if \"Sentence\" not in df.columns:\n",
    "    raise ValueError(\"The file must contain a column named 'Sentence'.\")\n",
    "\n",
    "# Load trained mBART model and tokenizer\n",
    "model_path = \"mbart_translation_small\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Set source and target languages\n",
    "tokenizer.src_lang = \"ru_RU\"\n",
    "target_lang_code = \"en_XX\"\n",
    "\n",
    "# Use GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Translation function with beam search\n",
    "def batch_translate(sentences, tokenizer, model, batch_size=16, num_beams=5):\n",
    "    translations = []\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "        batch = sentences[i:i + batch_size]\n",
    "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "        translated = model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tokenizer.convert_tokens_to_ids(target_lang_code),\n",
    "            num_beams=num_beams,\n",
    "            max_length=128,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        decoded = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "        translations.extend(decoded)\n",
    "    return translations\n",
    "\n",
    "# Translate all sentences\n",
    "sentences = df[\"Sentence\"].astype(str).tolist()\n",
    "translations = batch_translate(sentences, tokenizer, model)\n",
    "\n",
    "# Save translations\n",
    "df[\"Translation_mBART\"] = translations\n",
    "df.to_excel(\"translated_assembly_WER_mbart.xlsx\", index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(\"Translation completed using mBART. Output saved to 'translated_assembly_WER_mbart.xlsx'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training mBART with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883b9ba4553e44699cd64265606f4042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/309030 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658c78373002462fab1afa85c907770e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/38613 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_72207/294763683.py:62: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19312' max='48280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19312/48280 1:37:05 < 4:51:21, 1.66 it/s, Epoch 3/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>0.229390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.176800</td>\n",
       "      <td>0.234234</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved in 'mbart_translation_full'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load full training and validation data\n",
    "train_data = pd.read_excel(\"train_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "val_data = pd.read_excel(\"val_cleaned.xlsx\", engine=\"openpyxl\")\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "\n",
    "# Load tokenizer (original name is fine here)\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
    "tokenizer.src_lang = \"ru_RU\"\n",
    "tokenizer.tgt_lang = \"en_XX\"\n",
    "\n",
    "# Load model directly from checkpoint\n",
    "model = MBartForConditionalGeneration.from_pretrained(\"mbart_translation_full/checkpoint-9656\")\n",
    "\n",
    "# Preprocessing function using `text_target` (modern API)\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"Russian\"],\n",
    "        text_target=examples[\"English\"],\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Tokenize datasets\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"mbart_translation_full\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Resume training\n",
    "trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "# Save final model\n",
    "model.save_pretrained(\"mbart_translation_full\")\n",
    "tokenizer.save_pretrained(\"mbart_translation_full\")\n",
    "\n",
    "print(\"Training complete. Model saved in 'mbart_translation_full'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Translation using mBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 09:55:29.396922: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-26 09:55:29.477641: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742982929.515803      22 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742982929.528359      22 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742982929.601800      22 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742982929.601826      22 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742982929.601828      22 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742982929.601830      22 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-26 09:55:29.611395: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation completed and saved to 'translated_assembly_WER_mBART_bean10.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"assembly_WER.xlsx\"\n",
    "df = pd.read_excel(file_path, engine=\"openpyxl\")\n",
    "\n",
    "# Check that 'Sentence' column exists\n",
    "if \"Sentence\" not in df.columns:\n",
    "    raise ValueError(\"The file does not contain a 'Sentence' column. Please check the structure.\")\n",
    "\n",
    "# Load the trained mBART model and tokenizer\n",
    "model_path = \"mbart_translation_full\"\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_path)\n",
    "model = MBartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Set source and target languages\n",
    "tokenizer.src_lang = \"ru_RU\"\n",
    "tokenizer.tgt_lang = \"en_XX\"\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Translation function\n",
    "def batch_translate(sentences, model, tokenizer, num_beams=10):\n",
    "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs,\n",
    "        num_beams=num_beams,\n",
    "        max_length=128,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    return tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Perform translation in batches\n",
    "batch_size = 16\n",
    "sentences = df[\"Sentence\"].astype(str).tolist()\n",
    "translations = [\n",
    "    translation\n",
    "    for i in range(0, len(sentences), batch_size)\n",
    "    for translation in batch_translate(sentences[i:i + batch_size], model, tokenizer)\n",
    "]\n",
    "\n",
    "# Add translations to DataFrame\n",
    "df[\"Translation_mBART\"] = translations\n",
    "\n",
    "# Save to new Excel file\n",
    "output_file = \"translated_assembly_WER_mBART_bean10.xlsx\"\n",
    "df.to_excel(output_file, index=False, engine=\"openpyxl\")\n",
    "\n",
    "print(f\"Translation completed and saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
