{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a7bc80-72e9-468c-aa2b-a9c259c93b03",
   "metadata": {},
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ae1ead-1a81-49dc-a908-63e1f53d6391",
   "metadata": {},
   "source": [
    "def connect_to_database():\n",
    "    db_params = {\n",
    "    'host': '194.171.191.226',\n",
    "    'port': '6379',\n",
    "    'database': 'postgres',\n",
    "    'user': 'group6',\n",
    "    'password': 'blockd_2024group6_79'\n",
    "    }\n",
    "    try:\n",
    "        conn_psycopg2 = psycopg2.connect(**db_params)\n",
    "        print('Connection was successful!')\n",
    "        return conn_psycopg2\n",
    "    except Exception as e:\n",
    "        print('Connection was not successful!')\n",
    "        print(e)\n",
    "\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b13cff-da18-4c91-bae2-5e0f6a199287",
   "metadata": {},
   "source": [
    "\n",
    "def create_cursor(connection):\n",
    "    return connection.cursor()\n",
    "\n",
    "def close_cursor(cursor):\n",
    "    cursor.close()\n",
    "\n",
    "def close_connection(connection):\n",
    "    connection.close()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "211e4a36-f508-428f-8f33-189d894016cd",
   "metadata": {},
   "source": [
    "def init_database_connection(func):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def wrapper(*args):\n",
    "        connection = connect_to_database()\n",
    "\n",
    "        cursor = create_cursor(connection)\n",
    "        res =func(cursor ,*args)\n",
    "\n",
    "        close_cursor(cursor)\n",
    "        close_connection(connection)\n",
    "        return res\n",
    "\n",
    "    return wrapper"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d46662-4cbc-49f5-97b5-0d2c8528d267",
   "metadata": {},
   "source": [
    "@init_database_connection\n",
    "def show_select_query_results(cursor, query, show_results = False):\n",
    "   \n",
    "    cursor.execute(query)\n",
    " \n",
    "        \n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    if show_results:\n",
    "        print('Results are here' ,rows )\n",
    "        for row in rows:\n",
    "            print(row)\n",
    "        \n",
    "    return rows\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34bd0d17-3ca6-4466-b3e2-a47591a7a9ad",
   "metadata": {},
   "source": [
    "\n",
    "def get_column_names(table_name):\n",
    "        q = f'''\n",
    "    SELECT COLUMN_NAME\n",
    "    FROM information_schema.columns\n",
    "    WHERE table_schema ='data_lake'\n",
    "    AND table_name ='{table_name}'\n",
    "    ORDER BY ordinal_position\n",
    "    '''\n",
    "        return np.array(show_select_query_results(q)).flatten()\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18be14ae-e711-453b-91a4-71da1768d2bf",
   "metadata": {},
   "source": [
    "def load_sql_to_df(table_name):\n",
    "    col_names = get_column_names(table_name)\n",
    "\n",
    "    fetch_query = f'''\n",
    "     SELECT * FROM data_lake.{table_name}\n",
    "     ;\n",
    "\n",
    "    '''\n",
    "\n",
    "    result = show_select_query_results( fetch_query)\n",
    "\n",
    "    return pd.DataFrame( columns = col_names.tolist() , data  = result)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50f3a83e-b969-495f-8298-ed8c1c17ae9b",
   "metadata": {},
   "source": [
    "\n",
    "safe_driving_df = load_sql_to_df('safe_driving')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "46571751-95a7-44e1-8d3c-3bc430924858",
   "metadata": {},
   "source": [
    "We will ensure categorical values do not have uneccesary whitespace or other unexpected special characters, moreover, we will unifiy value casing to lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc2be36-5745-4fe8-ab11-7909f5d37bc2",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "def clean_categorical_data(df):\n",
    "     string_cols =  [ col  for col in df.columns  if 'object' == str(df[col].dtype)]\n",
    "\n",
    "     for col in string_cols:\n",
    "         df[col] = df[col].str.strip()\n",
    "         df[col] = df[col].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "         df[col] = df[col].str.lower()\n",
    "\n",
    "     for col in string_cols:\n",
    "        df = df.rename(columns={col: str(col).lower().replace(' ', '_')})\n",
    "\n",
    "\n",
    "          \n",
    "     return df\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b054b0f-1af0-4818-9035-67b15ae3be04",
   "metadata": {},
   "source": [
    "safe_driving_df = clean_categorical_data(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d35156e-1223-4c2a-862e-f53d0e390f38",
   "metadata": {},
   "source": [
    "#### checking missing values in dataset \n",
    "\n",
    "def print_line_break():\n",
    "    print('=' * 20 )\n",
    "    print(' ' + '-' * 18 + ' ')\n",
    "    print('=' * 20 )\n",
    "\n",
    "def show_dataframe_general_info(df):\n",
    "    display('General info of df')\n",
    "    display(df.info())\n",
    "    display('Description of df')\n",
    "    display(df.describe())\n",
    "    \n",
    "    check_df_missing_values(df)\n",
    "\n",
    "    \n",
    "\n",
    "def check_df_missing_values(df):\n",
    "    total_missing_values = df.isna().sum().sum()\n",
    "    display(f'Total number of missing values: ', total_missing_values)\n",
    "    \n",
    "    if total_missing_values > 0 :\n",
    "        \n",
    "        display('Number of missing values in particular columns: ')\n",
    "        display(df.isna().sum())\n",
    "\n",
    "\n",
    "def show_value_counts(df,col):\n",
    "    display(f'Value counts of {col}')\n",
    "    sorted_val_df = df[col].value_counts().sort_values(ascending = False)\n",
    "    \n",
    "    if sorted_val_df.shape[0] > 6:\n",
    "        sorted_val_df = sorted_val_df.iloc[:6]\n",
    "    display(sorted_val_df)\n",
    "\n",
    "    \n",
    "\n",
    "    print_line_break()\n",
    "    \n",
    "    display(f'Least used values in {col} column: ')\n",
    "    \n",
    "    display(df[col].value_counts().sort_values(ascending = True).iloc[:5])\n",
    "\n",
    "    unique_vals_in_col = len(pd.unique(df[col]))\n",
    "\n",
    "    col_dtype   = str(df[col].dtype)\n",
    "\n",
    "    \n",
    "\n",
    "    if col_dtype.startswith('int') or col_dtype.startswith('float'):\n",
    "        fig, ax  = plt.subplots(figsize =(14, 8))\n",
    "        sns.boxplot(x = col, data =df , ax =  ax)\n",
    "        plt.show()\n",
    "    elif  unique_vals_in_col < 20 and  col_dtype.startswith('object'):\n",
    "        fig, ax  = plt.subplots(figsize =(18, 8))\n",
    "        \n",
    "        missing_vals = df[col].isna().sum()\n",
    "\n",
    "        if missing_vals > 0:\n",
    "             ax.axhline(y  = missing_vals , color ='r' , linestyle='--' , linewidth = 2 , label='Missing values in df')\n",
    "             ax.legend()\n",
    "             sns.countplot( x = col , data = df.replace({np.nan:'unknown'}) , ax  =ax )\n",
    "        sns.countplot( x = col , data = df , ax  =ax )\n",
    "        \n",
    "        plt.show()\n",
    " \n",
    "        \n",
    "\n",
    " \n",
    "                \n",
    "    \n",
    "\n",
    "def show_dataframe_column_value_counts(df):\n",
    "    cols = df.columns\n",
    "\n",
    "    \n",
    "\n",
    "    for col in cols:\n",
    "        print_line_break()\n",
    "        show_value_counts(df,col)\n",
    "        missing_vals_in_col =df[col].isna().sum() \n",
    "        \n",
    "        if  missing_vals_in_col > 0:\n",
    "            display(f'Missing values in {col}')\n",
    "            display(f'{col}: {missing_vals_in_col}')\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baf0c317-6a13-4f73-a280-7523c8aec5a5",
   "metadata": {},
   "source": [
    "show_dataframe_general_info(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be354ac8-d76c-4381-8163-626956f4ecfb",
   "metadata": {},
   "source": [
    "show_dataframe_column_value_counts(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "badf2bc0-6b6f-4230-bc97-71f8d2b2548f",
   "metadata": {},
   "source": [
    "def show_duplicated_values_in_column(df , col_name):\n",
    "    display(\"Show duplicated values in column: {col_name}\")\n",
    "    total_duplicated_values  = df[col_name].duplicated().sum()\n",
    "    if total_duplicated_values > 0:\n",
    "        \n",
    "        display(f'Duplicated values in {col_name} :')\n",
    "        display('Number of duplicated values / all rows')\n",
    "        duplicated_values_perc = round((total_duplicated_values/df[col_name].shape[0]* 100) , 2)\n",
    "        \n",
    "        display(f'{total_duplicated_values}/{df[col_name].shape[0]} :  which is around {duplicated_values_perc}%')\n",
    "        sorted_val_df = df[col_name].value_counts().sort_values(ascending = False)\n",
    "        sorted_val_df = sorted_val_df[sorted_val_df > 1]\n",
    "\n",
    "        \n",
    "        display(sorted_val_df)\n",
    "\n",
    "        duplicated_values = sorted_val_df.reset_index()[col_name]\n",
    "\n",
    "\n",
    "\n",
    "        display('Show duplicated column rows :')\n",
    "        display(df[df[col_name].isin(duplicated_values.to_list())])\n",
    "    else:\n",
    "        display('No duplicated values in this column !!!')\n",
    "        \n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "978a95e7-b509-49e0-985b-e6808aceb7f9",
   "metadata": {},
   "source": [
    "def show_general_duplicate_values(df,col_name= None):\n",
    "\n",
    "    if col_name is not None:\n",
    "        \n",
    "        show_duplicated_values_in_column(df,col_name)\n",
    "    else:\n",
    "        total_duplicated_values  = df.duplicated().sum()\n",
    "        if total_duplicated_values > 0:\n",
    "            \n",
    "            display(f'Duplicated values in df:')\n",
    "            display('Number of duplicated values / all rows')\n",
    "            duplicated_values_perc = round((total_duplicated_values/df.shape[0]* 100) , 2)\n",
    "    \n",
    "            display(f'{total_duplicated_values}/{df.shape[0]} :  which is around {duplicated_values_perc}%')\n",
    "        else:\n",
    "            display('No duplicated values in this dataframe !!!')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def drop_duplicates_in_df(df,columns):\n",
    "   \n",
    "    drop_duplicated = False\n",
    "\n",
    "    if len(columns) > 1:\n",
    "        for col in columns:\n",
    "            display('Duplicated values in {columns} after dropping them')\n",
    "            print_line_break()\n",
    "            drop_duplicated = df.drop_duplicates(subset=[col] , inplace = True)\n",
    "            show_duplicated_values_in_column(safe_driving_df ,col )\n",
    "\n",
    "            print_line_break()\n",
    "    else:\n",
    "        display('Duplicated values in {columns} after dropping them')\n",
    "        print_line_break()\n",
    "        drop_duplicated = df.drop_duplicates(subset=[*columns] , inplace = True)\n",
    "        show_duplicated_values_in_column(safe_driving_df ,columns[0] )\n",
    "    \n",
    "    return drop_duplicated\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f45a9d0-7833-43b3-bddd-72f3f68aed6b",
   "metadata": {},
   "source": [
    "show_general_duplicate_values(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e9407b-4ee2-4565-b83d-66e396fec945",
   "metadata": {},
   "source": [
    "show_duplicated_values_in_column(safe_driving_df , 'eventid')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a91190a-2f6d-46a4-9b57-161056acadab",
   "metadata": {},
   "source": [
    "show_duplicated_values_in_column(safe_driving_df , 'event_start')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b49dafbe-5e5e-4cc9-8b56-261707679c9e",
   "metadata": {},
   "source": [
    "Since most of the duplicated id constitute the similar or the same accidents and the fraction of duplicated values is relatively insignificant, the duplicated rows will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8863746b-a90a-48ff-be51-5be78ee97329",
   "metadata": {},
   "source": [
    "drop_duplicates_in_df(safe_driving_df, ['eventid','event_start'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1d34a357-2b03-45ea-b353-5257dd3f845a",
   "metadata": {},
   "source": [
    "Now I will proceed to examine outliers \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1d1259b8-d55b-481d-90d8-da125a5c6182",
   "metadata": {},
   "source": [
    "safe_driving_df.dtypes"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a9566d3-3178-4f07-94ae-5bf43ede0282",
   "metadata": {},
   "source": [
    "def plot_columns(df,columns , plot):\n",
    "    if len(columns) == 0:\n",
    "        display('No columns to plot')\n",
    "        print_line_break()\n",
    "        print_line_break()\n",
    "        return\n",
    "\n",
    "    cols_length  = len(columns)\n",
    "    \n",
    "  \n",
    "    fig, axes = plt.subplots(nrows = cols_length , ncols = 1,figsize=(12, cols_length * 6) , sharex=False, sharey=False)\n",
    "    print(axes , type(axes))\n",
    "\n",
    "    if not isinstance(axes,np.ndarray):\n",
    "        axes = np.array([axes])\n",
    "        \n",
    "    for idx,  current_ax in enumerate(axes.flatten()):\n",
    "        if idx < len(columns):\n",
    "            \n",
    "          \n",
    "            current_col = columns[idx]\n",
    "            \n",
    "            current_ax.set_title(f'Column: {current_col}')\n",
    "            \n",
    "            plot(x = df[current_col] , ax = current_ax )\n",
    "\n",
    "    plt.show()\n",
    "            \n",
    "    \n",
    "\n",
    "def plot_numeric_columns(df, columns):\n",
    "    plot_columns(df,columns,sns.boxplot)\n",
    "\n",
    "  \n",
    "def plot_string_columns(df,columns):\n",
    "    plot_columns(df,columns,sns.countplot)\n",
    "\n",
    "def plot_bool_columns(df,columns):\n",
    "    plot_columns(df,columns,sns.countplot)\n",
    "\n",
    "\n",
    "def plot_value_distributions_in_df(df , columns_to_avoid = [] ):\n",
    "    \n",
    "    numeric_cols = [ col  for col in df.columns  if ( 'float' in str(df[col].dtype) or 'int' in str(df[col].dtype)) and  col not in columns_to_avoid ]\n",
    "\n",
    "    string_cols =   [ col  for col in df.columns  if 'object' == str(df[col].dtype)  and  col not in columns_to_avoid ]\n",
    "\n",
    "    bool_cols =   [ col  for col in df.columns  if 'bool' == str(df[col].dtype)  and  col not in columns_to_avoid ]\n",
    "\n",
    "    if numeric_cols:\n",
    "        \n",
    "\n",
    "        display('Numerical columns plotted :')\n",
    "        plot_numeric_columns(df, numeric_cols)\n",
    "        print_line_break()\n",
    "        print_line_break()\n",
    "        print_line_break()\n",
    "\n",
    "    if string_cols:\n",
    "        \n",
    "        display('String columns plotted :')\n",
    "        plot_string_columns(df, string_cols)\n",
    "    \n",
    "        print_line_break()\n",
    "        print_line_break()\n",
    "        print_line_break()\n",
    "\n",
    "    if bool_cols:\n",
    "        \n",
    "        \n",
    "        display('Bool columns plotted :')\n",
    "        plot_bool_columns(df,bool_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1871a51-231a-4b10-987f-f9247e92a6f9",
   "metadata": {},
   "source": [
    "plot_value_distributions_in_df(safe_driving_df , ['eventid', 'road_segment_id', 'latitude', 'longitude'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5ad37474-3cea-46a0-8f44-194a757b07d9",
   "metadata": {},
   "source": [
    "def show_outliers_fraction(df, col , Q1,Q3,IQR):\n",
    "        print_line_break()\n",
    "        display(f'The fraction of outliers in {col}')\n",
    "        total_outliers_number_in_col_mask =  (df[col] < Q1 - 1.5 * IQR) | (df[col] >  Q3 + 1.5 * IQR)\n",
    "        total_outliers_number_in_col  = df[total_outliers_number_in_col_mask].shape[0]\n",
    "        if total_outliers_number_in_col   <=0:\n",
    "            display(f'No outliers detected in {col} column')\n",
    "            return\n",
    "\n",
    "        print(total_outliers_number_in_col)\n",
    "        total_outliers_number_in_col_perc =  round( (total_outliers_number_in_col / df.shape[0])  , 2 ) * 100\n",
    "        display(f'{total_outliers_number_in_col}  / {df.shape[0]} which is around {total_outliers_number_in_col_perc}%')\n",
    "        print_line_break()\n",
    "\n",
    "def delete_outliers(df , columns , multiplier = 1.5):\n",
    "    df_no_outliers = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        Q1 = df_no_outliers[col].quantile(0.25)\n",
    "        Q3 = df_no_outliers[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "\n",
    "        lower_bound = Q1 - (IQR * multiplier)\n",
    "        upper_bound = Q3 + (IQR * multiplier)\n",
    "\n",
    "        show_outliers_fraction(df, col, Q1, Q3, IQR)\n",
    "        print(f\"{col}: Q1={Q1}, Q3={Q3}, IQR={IQR}, Lower Bound={lower_bound}, Upper Bound={upper_bound}\")\n",
    "\n",
    "        df_no_outliers = df_no_outliers[ (df_no_outliers[col] >=lower_bound) & (df_no_outliers[col] <= upper_bound)]\n",
    "\n",
    "    \n",
    "\n",
    "    return df_no_outliers"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "98c93457-46e2-4cf4-96d8-11507fb5a74f",
   "metadata": {},
   "source": [
    "Due to low amount of outliers located in dataset they will be removed using IQR and quantiles lower 0.25 and greater than 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f7f4e29-713e-48f0-8ca6-4109b18f8305",
   "metadata": {},
   "source": [
    "safe_driving_df = delete_outliers(safe_driving_df , ['end_speed_kmh' , 'speed_kmh' , 'duration_seconds'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b6f7ab9-b4d2-471e-89eb-295c819a6702",
   "metadata": {},
   "source": [
    "plot_value_distributions_in_df(safe_driving_df , ['eventid', 'road_segment_id', 'latitude', 'longitude'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2349f303-557a-4d72-a5df-5aed18b899ee",
   "metadata": {},
   "source": [
    "is_valid , road_manager_type, road_number,road_manager_name, municipality_name columns does not provide much value therefore they will be dropped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b989b84-b9ad-40b2-b74a-81f1ea454849",
   "metadata": {},
   "source": [
    "def drop_columns_in_df(df, columns_to_drop):\n",
    "    cols_drop_len  = len(columns_to_drop)\n",
    "\n",
    "    for col_to_drop in columns_to_drop:\n",
    "        if col_to_drop in df.columns:\n",
    "            \n",
    "            df.drop(columns=[col_to_drop] , inplace=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12cc200d-67e3-49c1-9313-d330c14c207a",
   "metadata": {},
   "source": [
    "\n",
    "drop_columns_in_df(safe_driving_df , ['is_valid', 'road_manager_type', 'road_number' , 'road_manager_name', 'municipality_name' , 'place_name'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdb93c5-81a0-4064-a590-6008abc73659",
   "metadata": {},
   "source": [
    "### Let's simplify incident_severity column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c61e56a-3d26-464f-903e-6ab3ba1357c2",
   "metadata": {},
   "source": [
    "\n",
    "def convert_column_to_binary(df, columns_with_new_values):\n",
    "    for key, val  in columns_with_new_values.items():\n",
    "        col = key\n",
    "        multiple_values = val['top_values']\n",
    "       \n",
    "        new_replace_value = val['new_value']\n",
    "        \n",
    "        most_frequent_values = df[col].value_counts().index[0:multiple_values]\n",
    "        \n",
    "        df[col] = df[col].apply(lambda row: row if str(row) in most_frequent_values else new_replace_value)\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d8773a0a-fa5f-4cb0-a62f-f434924d7d90",
   "metadata": {},
   "source": [
    "columns_with_new_values_dict = {\n",
    "    'incident_severity':{\n",
    "        'new_value':'other incident severities',\n",
    "         'top_values':2\n",
    "    },\n",
    "\n",
    "}\n",
    "\n",
    "convert_column_to_binary(safe_driving_df, columns_with_new_values_dict)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd11b717-c82e-4f9a-9582-a94fa9c235e0",
   "metadata": {},
   "source": [
    "plot_value_distributions_in_df(safe_driving_df , ['eventid', 'road_segment_id', 'latitude', 'longitude'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8a7d55d6-420e-4a5a-97fa-c16a23cfda38",
   "metadata": {},
   "source": [
    "Let's proceed with data inconsistencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f800820-4fbd-4fb5-8b05-8fd8f50e31ed",
   "metadata": {},
   "source": [
    "safe_driving_df['incident_severity'].value_counts()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e99fa6f2-47cd-4a8c-bbbf-fdafca5fcccf",
   "metadata": {},
   "source": [
    "def clip_numerical_cols(df,columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].round(2)\n",
    "\n",
    "\n",
    "def clean_numerical_cols(df):\n",
    "    numeric_cols = [ col  for col in df.columns  if ( 'float' in str(df[col].dtype) or 'int' in str(df[col].dtype)) ]\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        df[col] = df[col].abs()\n",
    "        df[col] = df[col].astype(float)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a2f1c26-3641-4492-b5fe-5121439d238f",
   "metadata": {},
   "source": [
    "clean_numerical_cols(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4ce92ec-158d-4ec5-a437-0918a33c8c3f",
   "metadata": {},
   "source": [
    "clip_numerical_cols(safe_driving_df,  ['speed_kmh' , 'end_speed_kmh', 'maxwaarde' ])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "198a2a43-f12b-463c-9c5d-d1368d58dd2c",
   "metadata": {},
   "source": [
    "There were some cases where initial speed_kmh was 0, we will analyze that\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e309269-835a-4c85-871b-22be9eea85b2",
   "metadata": {},
   "source": [
    "safe_driving_df[safe_driving_df['speed_kmh'] == 0.0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dae6fa85-7c12-437d-b852-33d4f741a7d2",
   "metadata": {},
   "source": [
    "These cases have reasonable explanation caused by Accelerating therefore these rows will not be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b176a7bb-25c4-433b-b7e6-05e23c9bf894",
   "metadata": {},
   "source": [
    "safe_driving_df.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "411fd595-d52a-4543-978d-cb524c56036a",
   "metadata": {},
   "source": [
    "Now we will prooced with scaling data using Standard Scaler from Sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b003c82-02b3-4434-80da-affeff2d9b99",
   "metadata": {},
   "source": [
    "def scale_numerical_data(df , columns):\n",
    "\n",
    "    for col in columns:\n",
    "        scaler = StandardScaler()\n",
    "        df[col] = scaler.fit_transform(df[[col]])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7c18b1ac-1581-4d14-bc2f-89aa0b38d7c7",
   "metadata": {},
   "source": [
    "scale_numerical_data(safe_driving_df , ['duration_seconds', 'speed_kmh', 'end_speed_kmh', 'maxwaarde'])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "82c400e8-21ab-4c66-ad36-98791c33b40c",
   "metadata": {},
   "source": [
    "safe_driving_df.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "af6983f1-5568-4cf1-88f6-8b10abf62c2a",
   "metadata": {},
   "source": [
    "Categorical encoding will be left for modelling process therefore no decoding functions will be implemented currently\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16613ed-a8f8-4999-970e-e527cd4ae969",
   "metadata": {},
   "source": [
    "Lets import weather informations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f557e7b-564d-4e40-8b3d-a58400d5619f",
   "metadata": {},
   "source": [
    "safe_driving_df = safe_driving_df.sort_values(by=['event_start'])\n",
    "safe_driving_df = safe_driving_df.iloc[:8000,:]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8418210f-e081-42f9-9533-d104de6839f9",
   "metadata": {},
   "source": [
    "def import_weather_df(table_name):\n",
    "    df = load_sql_to_df(table_name)\n",
    "    df = df.set_index('dtg').loc['2018-01-01':, :]\n",
    "    df = df.sort_index()\n",
    "    df = df.iloc[:10000,:]\n",
    "    \n",
    "    return df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cb16330-ce68-4e52-8de0-16881228f745",
   "metadata": {},
   "source": [
    "wind_df = import_weather_df('wind')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "4a1f9f9a-f7f2-434d-9309-65e56d9fd9da",
   "metadata": {},
   "source": [
    "No we will check basic column data distribution of ff sensor 10 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4da4c9a4-1c77-4d7e-802b-99af15b6bb25",
   "metadata": {},
   "source": [
    "wind_df['ff_sensor_10'].value_counts()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a4169778-42dc-46ac-a642-3321293fc531",
   "metadata": {},
   "source": [
    "sns.boxplot(x = wind_df['ff_sensor_10'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d626a25a-65a2-47b5-86e0-53cf9ce4bc31",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_average_value_optimized(accident_event_start, data_df, value_column):\n",
    "    # Adjust time range to numpy datetime64 for precise and efficient comparison\n",
    "    \n",
    "    time_interval_start = np.datetime64(accident_event_start - pd.Timedelta(hours=1))\n",
    "    accident_event_start = np.datetime64(accident_event_start)\n",
    "    \n",
    "    # Since 'dtg' is the index, we use .index for filtering\n",
    "    mask = (data_df.index >= time_interval_start) & (data_df.index <= accident_event_start)\n",
    "    filtered_values = data_df.loc[mask, value_column]  # Efficient filtering using .loc\n",
    "\n",
    "    # Calculate the mean using numpy to ensure minimal overhead, directly from Pandas series\n",
    "\n",
    "    \n",
    "   \n",
    "    return round(filtered_values.mean(),2) if not filtered_values.empty else np.nan\n",
    "\n",
    "def calculate_weather_statistics_optimized(weather_df, value_column, driving_df_values , new_value_column_name):\n",
    "    # Utilize list comprehension for efficient processing\n",
    "    \n",
    "    average_data = [\n",
    "        {'eventid': row_values[0], 'dtg': row_values[1], new_value_column_name: calculate_average_value_optimized(row_values[1], weather_df, value_column)}\n",
    "        for _, row_values in driving_df_values.iterrows()\n",
    "    ]\n",
    "    return pd.DataFrame(average_data)\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ab8f0db-b5b5-4360-85f5-4999bf816a8a",
   "metadata": {},
   "source": [
    "def merge_driving_with_weather_df(driving_df, weather_df,  old_value_column_name, new_value_column_name , on ='eventid', how='left'):\n",
    "    \n",
    "    average_weather_df = calculate_weather_statistics_optimized(weather_df, old_value_column_name, driving_df.loc[: , ['eventid' , 'event_start']] , new_value_column_name)\n",
    "\n",
    "    driving_df = pd.merge(driving_df, average_weather_df.loc[:,['eventid' , new_value_column_name]], on='eventid', how='left')\n",
    "\n",
    "    return driving_df"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ede14c3b-d284-4a14-97d8-b8992e433592",
   "metadata": {},
   "source": [
    "safe_driving_df = merge_driving_with_weather_df(safe_driving_df ,wind_df, 'ff_sensor_10',    'last_hour_wind_avg'   )\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58e866a8-ca19-4644-a8cf-4a746563ff8f",
   "metadata": {},
   "source": [
    "safe_driving_df.loc[: , ['event_start', 'last_hour_wind_avg']].describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c8a4c1aa-3d1f-453f-bb2b-b739205ff1da",
   "metadata": {},
   "source": [
    "temp_df = import_weather_df('temperature')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b994e359-ee9d-463c-a0c5-e250b9b1831c",
   "metadata": {},
   "source": [
    "temp_df.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67459279-4aff-4ed2-931c-bddfdb8d180d",
   "metadata": {},
   "source": [
    "\n",
    "safe_driving_df = merge_driving_with_weather_df(safe_driving_df ,temp_df, 't_dryb_10',    'last_hour_temp_avg' )\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "01e26dd4-0f48-4bb5-bebb-bf936949c014",
   "metadata": {},
   "source": [
    "prec_df = import_weather_df('precipitation')\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ff93191-da09-4f44-a732-4c8c300b7b53",
   "metadata": {},
   "source": [
    "\n",
    "safe_driving_df = merge_driving_with_weather_df(safe_driving_df ,prec_df, 'ri_pws_10',    'last_hour_rain_avg' )\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "546efcb4-91dc-4869-a3b9-bba79238257d",
   "metadata": {},
   "source": [
    "safe_driving_df.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4789a69b-1765-4ce9-a43c-78f97d492603",
   "metadata": {},
   "source": [
    "def scale_numerical_data(df , columns):\n",
    "\n",
    "    for col in columns:\n",
    "        scaler = StandardScaler()\n",
    "        df[col] = scaler.fit_transform(df[[col]])\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c6c287dc-7846-4585-8532-974d8673a7ba",
   "metadata": {},
   "source": [
    "scale_numerical_data(safe_driving_df , ['last_hour_wind_avg', 'last_hour_temp_avg', 'last_hour_rain_avg'])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d11271b8-4967-4dd7-a903-4b3991e4958e",
   "metadata": {},
   "source": [
    "safe_driving_df.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ccc6f3a-11be-4a39-956a-3cbd6ba2593c",
   "metadata": {},
   "source": [
    "safe_driving_df.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "48532b0d-1077-4622-9616-6c8ce789d4ba",
   "metadata": {},
   "source": [
    "### Import the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8e385f8-1ec5-459e-9e02-36669c5a6701",
   "metadata": {},
   "source": [
    "accidents_17_23_df = load_sql_to_df('accident_data_17_23')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1eadc3b4-8ebf-4a0b-b0f4-dff01ec4ff3e",
   "metadata": {},
   "source": [
    "### Manage columns and transform them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6548eb12-13a5-40cd-8618-91ac7043903c",
   "metadata": {},
   "source": [
    "def transform_numerical_column_to_str(df,columns):\n",
    "    df.loc[:, columns] = df.loc[:,columns].astype(str)\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5e056198-6200-46a2-86f7-e23eeeea7d3b",
   "metadata": {},
   "source": [
    "### We will transform Year column to categorical column in order to make it easier to plot for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18a7261c-7c15-437e-bd61-8ddef24b668a",
   "metadata": {},
   "source": [
    "transform_numerical_column_to_str( accidents_17_23_df , ['Year'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e2a64931-11c5-4c25-bc85-bba0b1dd5e7a",
   "metadata": {},
   "source": [
    "### We will ensure categorical values do not have uneccesary whitespace or other unexpected special characters, moreover, we will unifiy value casing to lower\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a43f504a-7cd1-469b-99a4-25a5f1c34f87",
   "metadata": {},
   "source": [
    "accidents_17_23_df = clean_categorical_data(accidents_17_23_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "828a175b-63bc-4a5a-b485-12728534578b",
   "metadata": {},
   "source": [
    "### We will ensure that all unknown values are converted as nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c80e67ce-d449-4606-95f5-3c1ea8bebd71",
   "metadata": {},
   "source": [
    "def convert_unknown_to_nan(df):\n",
    "    df.replace({'unknown':np.nan}, inplace = True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0b436e3-6405-447b-9b84-9aea689c8ac7",
   "metadata": {},
   "source": [
    " convert_unknown_to_nan(accidents_17_23_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "483531fb-1df5-413b-b7ad-31398bc9034b",
   "metadata": {},
   "source": [
    "### In some columns  are presents empty '' values which we will convert to  nan values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "395345c7-90ad-4296-8dc9-e55517544edd",
   "metadata": {},
   "source": [
    "def convert_empty_values_to_nan(df,columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].replace({'':np.nan})"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21a3968c-324a-47a7-88c0-42b78f487339",
   "metadata": {},
   "source": [
    "convert_empty_values_to_nan(accidents_17_23_df , ['first_mode_of_transport'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "dcdaf341-85ba-460e-aaf5-77505b243e0c",
   "metadata": {},
   "source": [
    "### We will ensure that rows with insignificant rows will be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3a2145f-798c-4679-bb78-f0e0f7d39de5",
   "metadata": {},
   "source": [
    "def drop_rows_with_drop_values(df, col  , drop_values):\n",
    "    \n",
    "    if drop_values:\n",
    "       mask = df[col].apply(lambda row: str(row) in drop_values)\n",
    "\n",
    "       print(pd.unique(mask))\n",
    "        \n",
    "       idxs_to_drop = df[mask].index\n",
    "       print(idxs_to_drop)\n",
    "       df.drop(index = idxs_to_drop , inplace = True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7a2bc7de-624b-4078-9f70-2ca76e3ba2bc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def convert_string_column_to_numerical(df,col ,  drop_values = []):\n",
    "    drop_rows_with_drop_values(df, col, drop_values)\n",
    "    \n",
    "    def return_speed(row):\n",
    "        \n",
    "        splitted_row =     str(row).split(' ')      \n",
    "        return float(splitted_row[0])\n",
    "        \n",
    "    df[col]  = df[col].apply(lambda row:  return_speed(row) if not pd.isnull(row) else row)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "0a0263cf-9377-4ff4-ad9d-d168eec45f45",
   "metadata": {},
   "source": [
    "### Delete footpace homezone value from speed_limit column because it only occurs 6 times in whole df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8deb3a87-ee75-43b7-a59e-9d96968e3ed0",
   "metadata": {},
   "source": [
    "convert_string_column_to_numerical(accidents_17_23_df, 'speed_limit' , drop_values = ['footpace  homezone']) "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6f5e3fb2-efce-404e-88fe-c6af9d59cbc1",
   "metadata": {},
   "source": [
    "### Drop municipality column because it has only \"breda\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f786ae04-96a0-4c43-859b-ef6192cf0bed",
   "metadata": {},
   "source": [
    "drop_columns_in_df(accidents_17_23_df , ['municipality'  ])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9d0897ef-8de8-4125-9b4a-a73b0e4718b2",
   "metadata": {},
   "source": [
    "### Show the columns with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0211db51-6a98-4d63-871d-3bfeb3556958",
   "metadata": {},
   "source": [
    "def show_columns_with_missing_values(df):\n",
    "    df_cols = df.columns\n",
    "\n",
    "    for col in df_cols:\n",
    "        missing_vals_in_col  = df[col].isna().sum()\n",
    "        if missing_vals_in_col > 0:\n",
    "            nan_perc = round( (missing_vals_in_col / df.shape[0]) * 100  , 2 )\n",
    "            print(f'Col: {col} has {missing_vals_in_col} missing values')\n",
    "            print(f'Percentage of missing values / all values in column: {nan_perc } %')\n",
    "            show_dataframe_column_value_counts(df[[col]])\n",
    "            print_line_break()\n",
    "            \n",
    "            \n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "02407a27-14e4-4b82-bb2f-6472718a10da",
   "metadata": {},
   "source": [
    "show_columns_with_missing_values(accidents_17_23_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "00a643f5-5735-4fd1-99bd-c0b5ce95d303",
   "metadata": {},
   "source": [
    "### Since every column has missing values over 15% that means that missing values constitute significant amount of important information , but because of the fact that I am not convinced how the datasets will be merged I will leave missing data imputation steps for later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f45175c6-1eb5-4077-9713-36eb2b0754d9",
   "metadata": {},
   "source": [
    "show_general_duplicate_values(accidents_17_23_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "06f8fd36-f3c4-4a83-b5d3-9e40ad2c97eb",
   "metadata": {},
   "source": [
    "### The brief analysis of dataset indicates no explicit unique identifier for each event in dataframe. Moreover, the amount of whole rows  duplicated is 0  therefore it is safe to assume that there are not any duplicates  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d28b7f8-4065-4e1c-9fb7-dd891264c635",
   "metadata": {},
   "source": [
    "### Now I will proceed to examine distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8b51a633-68a4-4021-bacf-ee28c6e769be",
   "metadata": {},
   "source": [
    "plot_value_distributions_in_df(accidents_17_23_df , columns_to_avoid = [])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6b4565f0-1269-486d-b212-0a3baff73d26",
   "metadata": {},
   "source": [
    "### Due to unequal distribution of certain columns let's convert them into binary column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1a4dd182-b841-460b-905c-9f7884f7226f",
   "metadata": {},
   "source": [
    "columns_with_new_values_dict = {\n",
    "    'accident_severity':{\n",
    "        'new_value':'injury or fatal',\n",
    "         'top_values':1\n",
    "    },\n",
    "    'town':{\n",
    "        'new_value':'other city',\n",
    "         'top_values':1\n",
    "    },\n",
    "\n",
    "    'first_mode_of_transport':{\n",
    "        'new_value':'other',\n",
    "         'top_values':1\n",
    "    },\n",
    "\n",
    "   'second_mode_of_transport':{\n",
    "        'new_value':'other',\n",
    "         'top_values':2\n",
    "    },\n",
    "\n",
    "    'light_condition':{\n",
    "        'new_value':'darkness or twilight',\n",
    "         'top_values':1\n",
    "    },\n",
    "    'road_condition':{\n",
    "        'new_value': 'wetdamp or snowblack ice',\n",
    "         'top_values':1\n",
    "    },\n",
    "    'road_situation':{\n",
    "        'new_value': 'other road situation',\n",
    "         'top_values':4\n",
    "    },\n",
    "    'weather':{\n",
    "        'new_value': 'other weather situation',\n",
    "         'top_values':2\n",
    "    },\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2aeddeb9-3368-4f3a-ac0d-4d5974f11d67",
   "metadata": {},
   "source": [
    "convert_column_to_binary(accidents_17_23_df ,columns_with_new_values_dict )\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f87f50da-0641-48c0-889e-f4492a20fae6",
   "metadata": {},
   "source": [
    "Let's eleminate outliers from speed_limit by removing accidents on road with very high or very low speed limit\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9215001b-dfb3-4139-a680-bb3ec95bd14c",
   "metadata": {},
   "source": [
    "def show_dist_for_cols(df , cols , boxplot = False):\n",
    "    fig , axes = plt.subplots( nrows = len(cols) ,ncols = 1 , figsize = (20 ,15))\n",
    "    if len(cols) == 1:\n",
    "        axes = np.array([axes])\n",
    "    for idx, ax in enumerate(axes.flatten()):\n",
    "    \n",
    "        if boxplot:\n",
    "            sns.boxplot(x=cols[idx] , ax = ax , data = df)\n",
    "        else:\n",
    "        \n",
    "            sns.countplot(x=cols[idx], ax = ax , data = df)\n",
    "\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8362fd28-5dc9-43f8-b2eb-9eb14f433aa0",
   "metadata": {},
   "source": [
    "show_dist_for_cols(accidents_17_23_df , ['speed_limit' , 'accidents'] , True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "89388897-0153-4061-990f-b220ee43bb18",
   "metadata": {},
   "source": [
    "accidents_17_23_df = delete_outliers(accidents_17_23_df , ['speed_limit' , 'accidents'] )\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bed1faf3-54b1-4041-8476-23c0acc31065",
   "metadata": {},
   "source": [
    "show_dist_for_cols(accidents_17_23_df , ['speed_limit' , 'accidents'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "722be163-8781-46fe-8fc3-709ad600f518",
   "metadata": {},
   "source": [
    "### Let's again present data distributions after transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "99331b4a-f7ef-4ef8-9e5a-adc42634c4b5",
   "metadata": {},
   "source": [
    "plot_value_distributions_in_df(accidents_17_23_df , columns_to_avoid = [])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5acca5e9-7412-4692-b04e-e0c399f86695",
   "metadata": {},
   "source": [
    "### Let's proceed with data inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c29e9e0-7864-4d4b-b941-c55e5f1c6d04",
   "metadata": {},
   "source": [
    "clean_numerical_cols(accidents_17_23_df) "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b21b33de-9239-4269-beb8-8853fed1842e",
   "metadata": {},
   "source": [
    "accidents_17_23_df.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bd8e6722-4a3d-4e8d-b519-2aa250a4b00f",
   "metadata": {},
   "source": [
    "### Let's see value counts of accidents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d51d4fe7-bbaa-4e45-b934-82eb39f7419d",
   "metadata": {},
   "source": [
    "accidents_17_23_df['accidents'].value_counts()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "19e5a5b8-3188-4770-9eb2-2a2a83efc5be",
   "metadata": {},
   "source": [
    "### Let's scale numerical values \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e0779915-4bfc-4b03-bde8-34d3357f37b1",
   "metadata": {},
   "source": [
    "scale_numerical_data(accidents_17_23_df , accidents_17_23_df.select_dtypes(include=['float' ,'int']))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "841e2962-4332-4b99-a5b5-ed8d59712017",
   "metadata": {},
   "source": [
    "accidents_17_23_df.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8972d3c3-9102-4af3-87a6-276e9a59e5a7",
   "metadata": {},
   "source": [
    "## Given the fact that accidents number has std = 0 and now only contains value 0 after transofrmations, no longer it constains meaningful info, therefore let's drop accidents columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38436a94-daa8-44d7-872f-ddb7cb84ae09",
   "metadata": {},
   "source": [
    "drop_columns_in_df(accidents_17_23_df , ['accidents'])\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c355b7d-6640-425b-91f6-9dc0527d340f",
   "metadata": {},
   "source": [
    "accidents_17_23_df.describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1b56c32b-bd37-4156-bd37-ff3f8d18bdae",
   "metadata": {},
   "source": [
    "### Categorical encoding will be left for modelling process therefore no decoding functions will be implemented currently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a9e79-3744-476e-831b-af2d8aee9809",
   "metadata": {},
   "source": [
    "## Now the data needs to be analyzed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2fe831-1f9a-4fbf-86a0-e40f9a7bdff0",
   "metadata": {},
   "source": [
    "### Let's make a weighted mean of accident_severity table to make it a new column for safe_driving_df\n",
    "So in our case the weighted mean of types of accidents severity will help us to assess if the street is high or low risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f4090d38-0594-4439-8454-2b8e2194d938",
   "metadata": {},
   "source": [
    "def transform_acc_sev_col_to_encoding(df):\n",
    "    df= df.copy()\n",
    "        \n",
    "    df = df.join(\n",
    "    pd.get_dummies(df['accident_severity'] ,  dtype=float))\n",
    "    return df\n",
    "  \n",
    "\n",
    "    \n",
    "def w_avg(row , weights):\n",
    "    w1,w2 = weights\n",
    "   \n",
    "    values_with_w_sum = row['injury_or_fatal_sum'] * w1 + row['material_damage_only_sum']  * w2\n",
    "    \n",
    "    return values_with_w_sum  / (w1 + w2) \n",
    "\n",
    "def calc_weighted_mean_of_acc_severity(df):\n",
    "    df = df.copy()\n",
    "    df = transform_acc_sev_col_to_encoding(df)\n",
    "\n",
    "    new_df  = df.groupby(['street']).agg(injury_or_fatal_sum= ('injury or fatal', 'sum'), material_damage_only_sum = ('material damage only' , 'sum')).reset_index()\n",
    "\n",
    "    new_df['weighted_avg'] = new_df.apply(lambda row: w_avg(row, [2,1]) , axis = 1)\n",
    "\n",
    "    print(new_df['weighted_avg'].describe())\n",
    "    return new_df\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b40888a0-2931-4c1f-93e0-c6f66518f2de",
   "metadata": {},
   "source": [
    "streets_with_accidents_ratio_df = calc_weighted_mean_of_acc_severity(accidents_17_23_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5224b453-a29c-4587-a05b-06fbe82948e5",
   "metadata": {},
   "source": [
    "streets_with_accidents_ratio_df.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c99d3ef0-84c9-478c-8067-5d96668838fb",
   "metadata": {},
   "source": [
    "### Let's merge safe_driving_df with strees with accidents ratio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc1d67fd-4d02-4a8c-bd8e-595f8b80861b",
   "metadata": {},
   "source": [
    "safe_driving_with_accidents_df = safe_driving_df.copy().merge(streets_with_accidents_ratio_df , how='left' , left_on='road_name', right_on='street')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "62aa3d75-9d92-49f1-bd54-b25261d603bf",
   "metadata": {},
   "source": [
    "### This will allow to create Y variable labeling for our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b091c37c-3c84-4936-b32c-15a7c9b4c0c0",
   "metadata": {},
   "source": [
    "plot_value_distributions_in_df(safe_driving_with_accidents_df[['weighted_avg']] , columns_to_avoid = [])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0d5f2853-d96e-43ad-bc3b-096ea3e9b93b",
   "metadata": {},
   "source": [
    "safe_driving_with_accidents_df[['weighted_avg']].describe()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f9d8142f-9629-4407-be7a-2602c82186d4",
   "metadata": {},
   "source": [
    "safe_driving_with_accidents_df['y_var'] = safe_driving_with_accidents_df['weighted_avg'].apply(lambda row:'low-risk'  if row <   safe_driving_with_accidents_df['weighted_avg'].mean() else 'high-risk')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bd040a6a-9ad6-4e57-8c53-9298a58043c0",
   "metadata": {},
   "source": [
    "### Let's drop columns which are not important after the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "41b4e64c-bfb7-43af-8e6e-2e0fe8752f71",
   "metadata": {},
   "source": [
    "drop_columns_in_df(safe_driving_with_accidents_df , ['street'])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a62244a8-d494-45c5-b935-73d9e0b57ef0",
   "metadata": {},
   "source": [
    "safe_driving_with_accidents_df.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "b6ef811f-59ff-4e32-9cc4-b544280799d2",
   "metadata": {},
   "source": [
    "### This is the dataframe for modelling:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "562d83bf-793a-4215-bdc9-c52ad019588e",
   "metadata": {},
   "source": [
    "safe_driving_with_accidents_df.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d7881ee0-a101-4349-b548-bddfa0ff9e8c",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "19d304e4-38b5-4069-8ce8-c2661fa4b5d4",
   "metadata": {},
   "source": [
    "# Show general information about the dataframe\n",
    "def show_dataframe_general_info(df):\n",
    "    print(\"General info of df\")\n",
    "    print(df.info())\n",
    "    print(\"Description of df\")\n",
    "    print(df.describe())\n",
    "\n",
    "show_dataframe_general_info(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1983da65-7091-4f53-9f88-9365763d5aab",
   "metadata": {},
   "source": [
    "# Check for missing values\n",
    "def check_df_missing_values(df):\n",
    "    total_missing_values = df.isna().sum().sum()\n",
    "    print(f\"Total number of missing values: {total_missing_values}\")\n",
    "    if total_missing_values > 0:\n",
    "        print(\"Number of missing values in particular columns:\")\n",
    "        print(df.isna().sum())\n",
    "\n",
    "check_df_missing_values(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5d123981-6e80-4dab-8fed-372ee9802aa7",
   "metadata": {},
   "source": [
    "# Define necessary numerical features\n",
    "important_numerical_features = [\n",
    "    'duration_seconds', 'latitude', 'longitude', \n",
    "    'speed_kmh', 'end_speed_kmh', 'maxwaarde', \n",
    "    'last_hour_wind_avg', 'last_hour_temp_avg', 'last_hour_rain_avg'\n",
    "]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e51db50-f7a6-4fd8-84f7-f26ef8dc2872",
   "metadata": {},
   "source": [
    "# Temporal analysis\n",
    "safe_driving_df['event_start'] = pd.to_datetime(safe_driving_df['event_start'])\n",
    "safe_driving_df['event_end'] = pd.to_datetime(safe_driving_df['event_end'])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "safe_driving_df['hour'] = safe_driving_df['event_start'].dt.hour\n",
    "sns.countplot(x='hour', data=safe_driving_df)\n",
    "plt.title('Incidents by Hour of Day')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "255c948d-5f2e-426f-9e2f-1dede9c70b15",
   "metadata": {},
   "source": [
    "# Plot value distributions\n",
    "def plot_value_distributions(df):\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for col in important_numerical_features:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(x=df[col])\n",
    "        plt.title(f'Box plot of {col}')\n",
    "        plt.show()\n",
    "\n",
    "plot_value_distributions(safe_driving_df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09f34ae2-e439-43c6-8c77-eab4c9e12c3e",
   "metadata": {},
   "source": [
    "# Correlation heatmap\n",
    "plt.figure(figsize=(16, 10))\n",
    "correlation_matrix = safe_driving_df[important_numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0a8ac639-9f91-4cf0-91e1-ec5f88514b53",
   "metadata": {},
   "source": [
    "# Pair plots for important numerical features\n",
    "sns.pairplot(safe_driving_df[important_numerical_features])\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ed53c2e0-494f-4e84-98f0-a061bfae993f",
   "metadata": {},
   "source": [
    "# Distribution of all categorical features\n",
    "\n",
    "categorical_cols = safe_driving_df.select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    if col == 'road_name':\n",
    "        top_20_roads = safe_driving_df['road_name'].value_counts().nlargest(20).index\n",
    "        sns.countplot(x=safe_driving_df[safe_driving_df['road_name'].isin(top_20_roads)]['road_name'])\n",
    "        plt.title('Distribution of Top 20 Road Names')\n",
    "    else:\n",
    "        sns.countplot(x=safe_driving_df[col])\n",
    "        plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1bb7b11f-90de-4699-890f-b984b5e350ba",
   "metadata": {},
   "source": [
    "# Box plots for numerical features across all categorical features\n",
    "categorical_cols = safe_driving_df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for cat_col in categorical_cols:\n",
    "    for num_col in important_numerical_features:\n",
    "        if num_col in safe_driving_df.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            if cat_col == 'road_name':\n",
    "                top_20_roads = safe_driving_df['road_name'].value_counts().nlargest(20).index\n",
    "                sns.boxplot(x=safe_driving_df[safe_driving_df['road_name'].isin(top_20_roads)][cat_col], y=safe_driving_df[num_col])\n",
    "                plt.title(f'{num_col} distribution across Top 20 {cat_col}')\n",
    "            else:\n",
    "                sns.boxplot(x=safe_driving_df[cat_col], y=safe_driving_df[num_col])\n",
    "                plt.title(f'{num_col} distribution across {cat_col}')\n",
    "            plt.xticks(rotation=90)\n",
    "            plt.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
